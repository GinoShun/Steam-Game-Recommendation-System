{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# detect gpus\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('../data/users.csv')\n",
    "games = pd.read_csv('../data/games.csv')\n",
    "\n",
    "games_metadata_list = []\n",
    "with open('../data/games_metadata.json', 'r') as f:\n",
    "    for line in f:\n",
    "        games_metadata_list.append(json.loads(line))\n",
    "games_metadata = pd.json_normalize(games_metadata_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### games_metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty descriptions: 10373\n",
      "Number of empty tags: 1244\n",
      "Rows with empty descriptions:\n",
      "        app_id description                                               tags\n",
      "1        22364                                                       [Action]\n",
      "7       271850              [Strategy, Simulation, Action, RTS, World War II]\n",
      "18      245950              [Action, RPG, FPS, Co-op, Shooter, Action RPG,...\n",
      "21      305181                                            [Adventure, Action]\n",
      "30      458790                          [Adventure, Soundtrack, Visual Novel]\n",
      "...        ...         ...                                                ...\n",
      "50866  2362300                                                             []\n",
      "50867  2296380                                                             []\n",
      "50868  1272080                                                             []\n",
      "50869  1402110                                                             []\n",
      "50871  2488510                                                             []\n",
      "\n",
      "[10373 rows x 3 columns]\n",
      "Rows with empty tags:\n",
      "        app_id description tags\n",
      "33      371970               []\n",
      "214    2277010               []\n",
      "219    1600150               []\n",
      "224     222573               []\n",
      "287     919640               []\n",
      "...        ...         ...  ...\n",
      "50866  2362300               []\n",
      "50867  2296380               []\n",
      "50868  1272080               []\n",
      "50869  1402110               []\n",
      "50871  2488510               []\n",
      "\n",
      "[1244 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 检查 description 列为空字符串的情况\n",
    "empty_description = games_metadata[games_metadata['description'] == '']\n",
    "print(f\"Number of empty descriptions: {len(empty_description)}\")\n",
    "\n",
    "# 检查 tags 列为空列表的情况\n",
    "empty_tags = games_metadata[games_metadata['tags'].apply(lambda x: len(x) == 0)]\n",
    "print(f\"Number of empty tags: {len(empty_tags)}\")\n",
    "\n",
    "# 打印出空 description 和空 tags 的行\n",
    "print(\"Rows with empty descriptions:\")\n",
    "print(empty_description)\n",
    "\n",
    "print(\"Rows with empty tags:\")\n",
    "print(empty_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games after dropping empty descriptions: 40499\n",
      "Number of games after dropping empty tags: 40484\n",
      "Number of unique tags: 441\n"
     ]
    }
   ],
   "source": [
    "# drop rows with empty tags and empty descriptions\n",
    "games_metadata = games_metadata[games_metadata['description'] != '']\n",
    "print(f\"Number of games after dropping empty descriptions: {len(games_metadata)}\")\n",
    "games_metadata = games_metadata[games_metadata['tags'].apply(lambda x: len(x) > 0)]\n",
    "print(f\"Number of games after dropping empty tags: {len(games_metadata)}\")\n",
    "games_metadata = games_metadata.reset_index(drop=True)\n",
    "\n",
    "all_tags = list(set(tag for tags in games_metadata['tags'] for tag in tags))\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit(all_tags)\n",
    "\n",
    "def encode_tags(tags_list):\n",
    "    return tag_encoder.transform(tags_list)\n",
    "\n",
    "games_metadata['tags_enc'] = games_metadata['tags'].apply(encode_tags)\n",
    "num_tags = len(tag_encoder.classes_)\n",
    "print(f\"Number of unique tags: {num_tags}\")\n",
    "\n",
    "# drop columns that are not needed\n",
    "games_metadata = games_metadata.drop(columns=['tags', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>tags_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13500</td>\n",
       "      <td>[17, 23, 271, 384, 170, 341, 283, 391, 38, 77,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113020</td>\n",
       "      <td>[79, 363, 192, 177, 210, 367, 266, 392, 17, 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226560</td>\n",
       "      <td>[439, 23, 374, 17, 384, 267, 167, 185, 341, 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249050</td>\n",
       "      <td>[318, 367, 395, 282, 192, 301, 329, 123, 23, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250180</td>\n",
       "      <td>[31, 77, 17, 79, 338, 314, 210, 335, 3, 266, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40479</th>\n",
       "      <td>2515240</td>\n",
       "      <td>[185, 38, 295, 23, 426, 150, 308, 105, 7, 167,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40480</th>\n",
       "      <td>2455060</td>\n",
       "      <td>[301, 192, 262, 20, 148, 17, 173, 23, 7, 320, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40481</th>\n",
       "      <td>1138640</td>\n",
       "      <td>[127, 208, 75, 191, 112, 58, 327, 221, 96, 44,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40482</th>\n",
       "      <td>1687000</td>\n",
       "      <td>[338, 49, 327, 223, 208, 301, 3, 366, 245, 267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40483</th>\n",
       "      <td>2272250</td>\n",
       "      <td>[127, 140, 17, 314, 150, 216, 38, 7, 335, 341,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40484 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        app_id                                           tags_enc\n",
       "0        13500  [17, 23, 271, 384, 170, 341, 283, 391, 38, 77,...\n",
       "1       113020  [79, 363, 192, 177, 210, 367, 266, 392, 17, 24...\n",
       "2       226560  [439, 23, 374, 17, 384, 267, 167, 185, 341, 24...\n",
       "3       249050  [318, 367, 395, 282, 192, 301, 329, 123, 23, 2...\n",
       "4       250180  [31, 77, 17, 79, 338, 314, 210, 335, 3, 266, 1...\n",
       "...        ...                                                ...\n",
       "40479  2515240  [185, 38, 295, 23, 426, 150, 308, 105, 7, 167,...\n",
       "40480  2455060  [301, 192, 262, 20, 148, 17, 173, 23, 7, 320, ...\n",
       "40481  1138640  [127, 208, 75, 191, 112, 58, 327, 221, 96, 44,...\n",
       "40482  1687000  [338, 49, 327, 223, 208, 301, 3, 366, 245, 267...\n",
       "40483  2272250  [127, 140, 17, 314, 150, 216, 38, 7, 335, 341,...\n",
       "\n",
       "[40484 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### games.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Very Positive' 'Positive' 'Mixed' 'Mostly Positive'\n",
      " 'Overwhelmingly Positive' 'Negative' 'Mostly Negative'\n",
      " 'Overwhelmingly Negative' 'Very Negative']\n"
     ]
    }
   ],
   "source": [
    "# check unique values in rating column\n",
    "print(games['rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5, 4, 6, 8, 2, 3, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map rating to numerical values ['Very Positive' 'Positive' 'Mixed' 'Mostly Positive' 'Overwhelmingly Positive' 'Negative' 'Mostly Negative' 'Overwhelmingly Negative' 'Very Negative']\n",
    "rating_map = {\n",
    "    'Overwhelmingly Positive': 8,\n",
    "    'Very Positive': 7,\n",
    "    'Mostly Positive': 6,\n",
    "    'Positive': 5,\n",
    "    'Mixed': 4,\n",
    "    'Mostly Negative': 3,\n",
    "    'Negative': 2,\n",
    "    'Overwhelmingly Negative': 1,\n",
    "    'Very Negative': 0\n",
    "}\n",
    "\n",
    "games['rating'] = games['rating'].map(rating_map)\n",
    "games['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   app_id                              title  win  mac  linux  rating  \\\n",
      "0   13500  Prince of Persia: Warrior Within™    1    0      0       7   \n",
      "1   22364            BRINK: Agents of Change    1    0      0       5   \n",
      "2  113020       Monaco: What's Yours Is Mine    1    1      1       7   \n",
      "3  226560                 Escape Dead Island    1    0      0       4   \n",
      "4  249050            Dungeon of the ENDLESS™    1    1      0       7   \n",
      "\n",
      "   positive_ratio  user_reviews  price_final  price_original  discount  \\\n",
      "0            84.0          2199         9.99            9.99       0.0   \n",
      "1            85.0            21         2.99            2.99       0.0   \n",
      "2            92.0          3722        14.99           14.99       0.0   \n",
      "3            61.0           873        14.99           14.99       0.0   \n",
      "4            88.0          8784        11.99           11.99       0.0   \n",
      "\n",
      "   steam_deck  release_days  \n",
      "0           1          5801  \n",
      "1           1          4816  \n",
      "2           1          4186  \n",
      "3           1          3613  \n",
      "4           1          3635  \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "games['app_id'] = games['app_id'].astype(int)\n",
    "\n",
    "games['date_release'] = pd.to_datetime(games['date_release'])\n",
    "current_date = datetime.now()\n",
    "games['release_days'] = (current_date - games['date_release']).dt.days\n",
    "games.drop('date_release', axis=1, inplace=True) \n",
    "\n",
    "games['win'] = games['win'].astype(int)\n",
    "games['mac'] = games['mac'].astype(int)\n",
    "games['linux'] = games['linux'].astype(int)\n",
    "\n",
    "games['positive_ratio'] = games['positive_ratio'].astype(float)\n",
    "games['user_reviews'] = games['user_reviews'].astype(int)\n",
    "\n",
    "games['price_final'] = games['price_final'].astype(float)\n",
    "games['price_original'] = games['price_original'].astype(float)\n",
    "games['discount'] = games['discount'].astype(float)\n",
    "\n",
    "games['steam_deck'] = games['steam_deck'].astype(int)\n",
    "\n",
    "print(games.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               title  \\\n",
      "0  Prince of Persia: Warrior Within™   \n",
      "1            BRINK: Agents of Change   \n",
      "2       Monaco: What's Yours Is Mine   \n",
      "3                 Escape Dead Island   \n",
      "4            Dungeon of the ENDLESS™   \n",
      "\n",
      "                                        title_vector  \n",
      "0  [0.1686721383397911, 0.37316500696018323, -0.0...  \n",
      "1  [0.40562555982970794, 0.9066350280974034, -0.0...  \n",
      "2  [0.005065994732817841, -0.0029820103022997163,...  \n",
      "3  [0.011830614796189916, -0.000649821763741745, ...  \n",
      "4  [0.3920505435491173, 0.17049008517470388, -0.0...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. 使用TF-IDF来提取标题中的特征\n",
    "vectorizer = TfidfVectorizer(max_features=300)  # 限制最大特征数为300，以控制稀疏度\n",
    "X_tfidf = vectorizer.fit_transform(games['title'])\n",
    "\n",
    "# 2. 使用TruncatedSVD对TF-IDF矩阵进行降维，减少稀疏度\n",
    "svd = TruncatedSVD(n_components=50)  # 降维到50维\n",
    "X_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "# 将降维后的结果加入到数据集中\n",
    "games['title_vector'] = list(X_reduced)\n",
    "\n",
    "# 检查处理后的结果\n",
    "print(games[['title', 'title_vector']].head())\n",
    "games.drop('title', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>win</th>\n",
       "      <th>mac</th>\n",
       "      <th>linux</th>\n",
       "      <th>rating</th>\n",
       "      <th>positive_ratio</th>\n",
       "      <th>user_reviews</th>\n",
       "      <th>price_final</th>\n",
       "      <th>price_original</th>\n",
       "      <th>discount</th>\n",
       "      <th>steam_deck</th>\n",
       "      <th>release_days</th>\n",
       "      <th>title_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2199</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5801</td>\n",
       "      <td>[0.1686721383397911, 0.37316500696018323, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22364</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>21</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4816</td>\n",
       "      <td>[0.40562555982970794, 0.9066350280974034, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3722</td>\n",
       "      <td>14.99</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4186</td>\n",
       "      <td>[0.005065994732817841, -0.0029820103022997163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226560</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>873</td>\n",
       "      <td>14.99</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3613</td>\n",
       "      <td>[0.011830614796189916, -0.000649821763741745, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8784</td>\n",
       "      <td>11.99</td>\n",
       "      <td>11.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3635</td>\n",
       "      <td>[0.3920505435491173, 0.17049008517470388, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50867</th>\n",
       "      <td>2296380</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>96.0</td>\n",
       "      <td>101</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>377</td>\n",
       "      <td>[0.2344836556006683, -0.10779081683952314, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50868</th>\n",
       "      <td>1272080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29458</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>384</td>\n",
       "      <td>[0.002217280834058938, -0.0010282502199896245,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50869</th>\n",
       "      <td>1402110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1128</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>394</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50870</th>\n",
       "      <td>2272250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>95.0</td>\n",
       "      <td>82</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>[0.001694479232026515, -0.00023963491084013508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50871</th>\n",
       "      <td>2488510</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>88.0</td>\n",
       "      <td>144</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>352</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50872 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        app_id  win  mac  linux  rating  positive_ratio  user_reviews  \\\n",
       "0        13500    1    0      0       7            84.0          2199   \n",
       "1        22364    1    0      0       5            85.0            21   \n",
       "2       113020    1    1      1       7            92.0          3722   \n",
       "3       226560    1    0      0       4            61.0           873   \n",
       "4       249050    1    1      0       7            88.0          8784   \n",
       "...        ...  ...  ...    ...     ...             ...           ...   \n",
       "50867  2296380    1    0      0       7            96.0           101   \n",
       "50868  1272080    1    0      0       3            38.0         29458   \n",
       "50869  1402110    1    0      0       7            89.0          1128   \n",
       "50870  2272250    1    0      0       7            95.0            82   \n",
       "50871  2488510    1    0      0       7            88.0           144   \n",
       "\n",
       "       price_final  price_original  discount  steam_deck  release_days  \\\n",
       "0             9.99            9.99       0.0           1          5801   \n",
       "1             2.99            2.99       0.0           1          4816   \n",
       "2            14.99           14.99       0.0           1          4186   \n",
       "3            14.99           14.99       0.0           1          3613   \n",
       "4            11.99           11.99       0.0           1          3635   \n",
       "...            ...             ...       ...         ...           ...   \n",
       "50867        22.00            0.00       0.0           1           377   \n",
       "50868        40.00            0.00       0.0           1           384   \n",
       "50869        30.00            0.00       0.0           1           394   \n",
       "50870        17.00            0.00       0.0           1           356   \n",
       "50871         4.00            0.00       0.0           1           352   \n",
       "\n",
       "                                            title_vector  \n",
       "0      [0.1686721383397911, 0.37316500696018323, -0.0...  \n",
       "1      [0.40562555982970794, 0.9066350280974034, -0.0...  \n",
       "2      [0.005065994732817841, -0.0029820103022997163,...  \n",
       "3      [0.011830614796189916, -0.000649821763741745, ...  \n",
       "4      [0.3920505435491173, 0.17049008517470388, -0.0...  \n",
       "...                                                  ...  \n",
       "50867  [0.2344836556006683, -0.10779081683952314, 0.0...  \n",
       "50868  [0.002217280834058938, -0.0010282502199896245,...  \n",
       "50869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "50870  [0.001694479232026515, -0.00023963491084013508...  \n",
       "50871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[50872 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recommendations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>helpful</th>\n",
       "      <th>funny</th>\n",
       "      <th>date</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>hours</th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>True</td>\n",
       "      <td>36.3</td>\n",
       "      <td>51580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304390</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-17</td>\n",
       "      <td>False</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1085660</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-17</td>\n",
       "      <td>True</td>\n",
       "      <td>336.5</td>\n",
       "      <td>253880</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>703080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>True</td>\n",
       "      <td>27.4</td>\n",
       "      <td>259432</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>526870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>True</td>\n",
       "      <td>7.9</td>\n",
       "      <td>23869</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154789</th>\n",
       "      <td>633230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-15</td>\n",
       "      <td>True</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1606890</td>\n",
       "      <td>41154789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154790</th>\n",
       "      <td>758870</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1786254</td>\n",
       "      <td>41154790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154791</th>\n",
       "      <td>696170</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6370324</td>\n",
       "      <td>41154791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154792</th>\n",
       "      <td>696170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1044289</td>\n",
       "      <td>41154792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154793</th>\n",
       "      <td>1089980</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>True</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13971935</td>\n",
       "      <td>41154793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41154794 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           app_id  helpful  funny        date  is_recommended  hours  \\\n",
       "0          975370        0      0  2022-12-12            True   36.3   \n",
       "1          304390        4      0  2017-02-17           False   11.5   \n",
       "2         1085660        2      0  2019-11-17            True  336.5   \n",
       "3          703080        0      0  2022-09-23            True   27.4   \n",
       "4          526870        0      0  2021-01-10            True    7.9   \n",
       "...           ...      ...    ...         ...             ...    ...   \n",
       "41154789   633230        0      0  2021-02-15            True   41.0   \n",
       "41154790   758870        8      0  2019-07-18           False    8.0   \n",
       "41154791   696170        3     10  2018-03-26           False    2.0   \n",
       "41154792   696170        0      0  2018-06-11            True    4.0   \n",
       "41154793  1089980        2      0  2020-09-16            True   14.0   \n",
       "\n",
       "           user_id  review_id  \n",
       "0            51580          0  \n",
       "1             2586          1  \n",
       "2           253880          2  \n",
       "3           259432          3  \n",
       "4            23869          4  \n",
       "...            ...        ...  \n",
       "41154789   1606890   41154789  \n",
       "41154790   1786254   41154790  \n",
       "41154791   6370324   41154791  \n",
       "41154792   1044289   41154792  \n",
       "41154793  13971935   41154793  \n",
       "\n",
       "[41154794 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations = pd.read_csv('../data/recommendations.csv')\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinuo\\AppData\\Local\\Temp\\ipykernel_28256\\959098349.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 1 ... 0 1 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  recommendations.loc[:, 'is_recommended'] = recommendations['is_recommended'].map({True: 1, False: 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    app_id  helpful  funny  is_recommended  hours  user_id  year  month  day  \\\n",
      "0   975370        0      0               1   36.3    51580  2022     12   12   \n",
      "1   304390        4      0               0   11.5     2586  2017      2   17   \n",
      "2  1085660        2      0               1  336.5   253880  2019     11   17   \n",
      "3   703080        0      0               1   27.4   259432  2022      9   23   \n",
      "4   526870        0      0               1    7.9    23869  2021      1   10   \n",
      "\n",
      "   weekday  \n",
      "0        0  \n",
      "1        4  \n",
      "2        6  \n",
      "3        4  \n",
      "4        6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "recommendations['date'] = pd.to_datetime(recommendations['date'], format='%Y-%m-%d')\n",
    "recommendations['year'] = recommendations['date'].dt.year\n",
    "recommendations['month'] = recommendations['date'].dt.month\n",
    "recommendations['day'] = recommendations['date'].dt.day\n",
    "recommendations['weekday'] = recommendations['date'].dt.weekday  # 周一为0，周日为6\n",
    "\n",
    "recommendations.drop(columns=['date'], inplace=True)\n",
    "\n",
    "recommendations.loc[:, 'helpful'] = recommendations['helpful'].astype(int)\n",
    "recommendations.loc[:, 'funny'] = recommendations['funny'].astype(int)\n",
    "recommendations.loc[:, 'hours'] = recommendations['hours'].astype(float)\n",
    "# delete hours <= 2.0\n",
    "recommendations = recommendations[recommendations['hours'] > 2.0]\n",
    "\n",
    "recommendations.loc[:, 'is_recommended'] = recommendations['is_recommended'].map({True: 1, False: 0})\n",
    "\n",
    "recommendations = recommendations.drop(columns=['review_id'])\n",
    "\n",
    "print(recommendations.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>helpful</th>\n",
       "      <th>funny</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>hours</th>\n",
       "      <th>user_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36.3</td>\n",
       "      <td>51580</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304390</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2586</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1085660</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>336.5</td>\n",
       "      <td>253880</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>703080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.4</td>\n",
       "      <td>259432</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>526870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>23869</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154788</th>\n",
       "      <td>391220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9958247</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154789</th>\n",
       "      <td>633230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1606890</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154790</th>\n",
       "      <td>758870</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1786254</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154792</th>\n",
       "      <td>696170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1044289</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154793</th>\n",
       "      <td>1089980</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13971935</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37060476 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           app_id  helpful  funny  is_recommended  hours   user_id  year  \\\n",
       "0          975370        0      0               1   36.3     51580  2022   \n",
       "1          304390        4      0               0   11.5      2586  2017   \n",
       "2         1085660        2      0               1  336.5    253880  2019   \n",
       "3          703080        0      0               1   27.4    259432  2022   \n",
       "4          526870        0      0               1    7.9     23869  2021   \n",
       "...           ...      ...    ...             ...    ...       ...   ...   \n",
       "41154788   391220        0      0               1   18.0   9958247  2021   \n",
       "41154789   633230        0      0               1   41.0   1606890  2021   \n",
       "41154790   758870        8      0               0    8.0   1786254  2019   \n",
       "41154792   696170        0      0               1    4.0   1044289  2018   \n",
       "41154793  1089980        2      0               1   14.0  13971935  2020   \n",
       "\n",
       "          month  day  weekday  \n",
       "0            12   12        0  \n",
       "1             2   17        4  \n",
       "2            11   17        6  \n",
       "3             9   23        4  \n",
       "4             1   10        6  \n",
       "...         ...  ...      ...  \n",
       "41154788      4   28        2  \n",
       "41154789      2   15        0  \n",
       "41154790      7   18        3  \n",
       "41154792      6   11        0  \n",
       "41154793      9   16        2  \n",
       "\n",
       "[37060476 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendations.to_csv('../data/processed/preprocessed_recommendations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>products</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7360263</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14020781</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8762579</td>\n",
       "      <td>329</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4820647</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5167327</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14306059</th>\n",
       "      <td>5047430</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14306060</th>\n",
       "      <td>5048153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14306061</th>\n",
       "      <td>5059205</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14306062</th>\n",
       "      <td>5074363</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14306063</th>\n",
       "      <td>5081164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14306064 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id  products  reviews\n",
       "0          7360263       359        0\n",
       "1         14020781       156        1\n",
       "2          8762579       329        4\n",
       "3          4820647       176        4\n",
       "4          5167327        98        2\n",
       "...            ...       ...      ...\n",
       "14306059   5047430         6        0\n",
       "14306060   5048153         0        0\n",
       "14306061   5059205        31        0\n",
       "14306062   5074363         0        0\n",
       "14306063   5081164         0        0\n",
       "\n",
       "[14306064 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users without reviews: 1051082\n",
      "Number of users without games: 139318\n"
     ]
    }
   ],
   "source": [
    "# check how many users have 0 reviews(in recommendations.csv)\n",
    "users_with_reviews = recommendations['user_id'].unique()\n",
    "users_without_reviews = users[~users['user_id'].isin(users_with_reviews)]\n",
    "print(f\"Number of users without reviews: {len(users_without_reviews)}\")\n",
    "\n",
    "# check how many users have 0 games\n",
    "# only look at products column that <= 0 in users.csv\n",
    "users_with_games = users[users['products'] > 0]\n",
    "users_without_games = users[~users['user_id'].isin(users_with_games['user_id'])]\n",
    "print(f\"Number of users without games: {len(users_without_games)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>products</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14020781</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8762579</td>\n",
       "      <td>329</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4820647</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5167327</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5664667</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13139325</th>\n",
       "      <td>4897585</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13139326</th>\n",
       "      <td>4898597</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13139327</th>\n",
       "      <td>4902361</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13139328</th>\n",
       "      <td>4902380</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13139329</th>\n",
       "      <td>4935868</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13139330 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id  products  reviews\n",
       "0         14020781       156        1\n",
       "1          8762579       329        4\n",
       "2          4820647       176        4\n",
       "3          5167327        98        2\n",
       "4          5664667       145        5\n",
       "...            ...       ...      ...\n",
       "13139325   4897585        54        2\n",
       "13139326   4898597         7        1\n",
       "13139327   4902361         7        1\n",
       "13139328   4902380         9        1\n",
       "13139329   4935868        39        1\n",
       "\n",
       "[13139330 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop users without reviews and games\n",
    "users = users[users['user_id'].isin(users_with_reviews)]\n",
    "users = users[users['user_id'].isin(users_with_games['user_id'])]\n",
    "users = users.reset_index(drop=True)\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_full = pd.merge(games, games_metadata, on='app_id', how='inner')\n",
    "\n",
    "data = pd.merge(recommendations, games_full, on='app_id', how='inner')\n",
    "\n",
    "data = pd.merge(data, users, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    app_id  helpful  funny  is_recommended  hours  user_id  year  month  day  \\\n",
      "0  1544020       18      2               0    9.1  9254726  2022     12    2   \n",
      "\n",
      "   weekday  win  mac  linux  rating  positive_ratio  user_reviews  \\\n",
      "0        4    1    0      0       4            62.0         21737   \n",
      "\n",
      "   price_final  price_original  discount  steam_deck  release_days  \\\n",
      "0        59.99           59.99       0.0           1           678   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              title_vector  \\\n",
      "0  [0.9077150941072197, -0.41081309319580617, -0.02329612338111697, -0.004637117381524289, -0.015528420815487769, -0.04089551496357709, -0.011162745504591108, -0.009822665958539204, 0.001261948739425603, -0.02930881833820208, -0.02324772457289689, -0.03034211710265456, -0.013752630572286479, 0.000690215659481318, 0.005193681151831822, -0.00910844273855153, -0.0008894088002635511, -0.0027172072200979183, 0.0003711807034643376, -0.0055584399863470475, -0.008293602633723763, -0.006867355912923883, -0.00828552389867114, -0.005539471409886704, -0.00674174148016496, -0.009890379123204267, -0.00016027843287659375, -0.021898878488652657, 0.00873214384070901, 0.0030253744189371393, -0.0007225464682931552, -0.0033143045238072802, -0.00022545192971521084, -0.00150819023246617, 0.0006921867629453033, 0.001394863823904108, -0.001764142398423995, -0.002106080831910613, -0.002264289261084143, -0.003603082000843339, -0.0017419329566172766, -0.0023199943309871185, 0.0017850478108770093, -0.0020687019481218302, -0.0013230126770212638, 0.0011870424665144236, 0.0022754312959339246, 0.0001255740024670854, -0.003177830201025522, 0.004146963729212763]   \n",
      "\n",
      "                                                                                         tags_enc  \\\n",
      "0  [185, 329, 375, 385, 356, 335, 21, 384, 298, 7, 17, 366, 209, 308, 23, 316, 74, 105, 159, 334]   \n",
      "\n",
      "   products  reviews  \n",
      "0       724       34  \n",
      "app_id              int64\n",
      "helpful             int64\n",
      "funny               int64\n",
      "is_recommended      int64\n",
      "hours             float64\n",
      "user_id             int64\n",
      "year                int32\n",
      "month               int32\n",
      "day                 int32\n",
      "weekday             int32\n",
      "win                 int32\n",
      "mac                 int32\n",
      "linux               int32\n",
      "rating              int64\n",
      "positive_ratio    float64\n",
      "user_reviews        int32\n",
      "price_final       float64\n",
      "price_original    float64\n",
      "discount          float64\n",
      "steam_deck          int32\n",
      "release_days        int64\n",
      "title_vector       object\n",
      "tags_enc           object\n",
      "products            int64\n",
      "reviews             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print first row in full text\n",
    "pd.set_option('display.max_columns', None)\n",
    "# make sure column content(such as list) are also fully displayed\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(data.head(1))\n",
    "# print type of each column\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check detailed type of tags_enc and title_vector\n",
    "print(type(data['tags_enc'][0]))\n",
    "print(type(data['title_vector'][0]))\n",
    "\n",
    "# # convert tags_enc and title_vector to list\n",
    "# data['tags_enc'] = data['tags_enc'].apply(lambda x: list(x))\n",
    "# data['title_vector'] = data['title_vector'].apply(lambda x: list(x))\n",
    "\n",
    "# # check detailed type of tags_enc and title_vector\n",
    "# print(type(data['tags_enc'][0]))\n",
    "# print(type(data['title_vector'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_id            0\n",
      "helpful           0\n",
      "funny             0\n",
      "is_recommended    0\n",
      "hours             0\n",
      "user_id           0\n",
      "year              0\n",
      "month             0\n",
      "day               0\n",
      "weekday           0\n",
      "win               0\n",
      "mac               0\n",
      "linux             0\n",
      "rating            0\n",
      "positive_ratio    0\n",
      "user_reviews      0\n",
      "price_final       0\n",
      "price_original    0\n",
      "discount          0\n",
      "steam_deck        0\n",
      "release_days      0\n",
      "title_vector      0\n",
      "tags_enc          0\n",
      "products          0\n",
      "reviews           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check if there are any missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed full dataset\n",
    "# data.to_csv('../data/processed/processed_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>helpful</th>\n",
       "      <th>funny</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>hours</th>\n",
       "      <th>user_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>user_reviews</th>\n",
       "      <th>price_final</th>\n",
       "      <th>price_original</th>\n",
       "      <th>discount</th>\n",
       "      <th>steam_deck</th>\n",
       "      <th>release_days</th>\n",
       "      <th>title_vector</th>\n",
       "      <th>tags_enc</th>\n",
       "      <th>products</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1544020</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9254726</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>21737</td>\n",
       "      <td>59.99</td>\n",
       "      <td>59.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>678</td>\n",
       "      <td>[0.9077150940715599, -0.410813095011428, -0.02...</td>\n",
       "      <td>[185, 329, 375, 385, 356, 335, 21, 384, 298, 7...</td>\n",
       "      <td>724</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    app_id  helpful  funny  is_recommended  hours  user_id  year  month  day  \\\n",
       "0  1544020       18      2               0    9.1  9254726  2022     12    2   \n",
       "\n",
       "   weekday  ...  user_reviews  price_final  price_original  discount  \\\n",
       "0        4  ...         21737        59.99           59.99       0.0   \n",
       "\n",
       "   steam_deck  release_days  \\\n",
       "0           1           678   \n",
       "\n",
       "                                        title_vector  \\\n",
       "0  [0.9077150940715599, -0.410813095011428, -0.02...   \n",
       "\n",
       "                                            tags_enc  products  reviews  \n",
       "0  [185, 329, 375, 385, 356, 335, 21, 384, 298, 7...       724       34  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shut down full text display\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_colwidth')\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['user_id', 'app_id', 'helpful', 'funny', 'hours', 'year', 'month', 'day', 'weekday',\n",
    "                   'win', 'mac', 'linux', 'rating', 'positive_ratio', 'user_reviews', 'price_final',\n",
    "                   'price_original', 'discount', 'steam_deck', 'release_days', 'products', 'reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # 将 numpy 数组展开并添加到特征列中\n",
    "# title_vector_array = np.stack(data['title_vector'].values)\n",
    "# # 找到最长的 tags_enc 数组长度\n",
    "# max_len = max(len(x) for x in data['tags_enc'].values)\n",
    "# # 填充 tags_enc 数组\n",
    "# tags_enc_array = np.array([np.pad(x, (0, max_len - len(x)), 'constant') for x in data['tags_enc'].values])\n",
    "\n",
    "# # 合并所有特征\n",
    "# X_numeric = data[feature_columns].values\n",
    "# X = np.hstack((X_numeric, title_vector_array, tags_enc_array))\n",
    "\n",
    "# # 目标变量\n",
    "# y = data['is_recommended'].values\n",
    "\n",
    "# # 划分数据集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=['user_id', 'year', 'month', 'day'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only keep users with strong actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 预处理 'tags_enc' 列，填充为相同的长度\n",
    "max_len = max(len(tags) for tags in data['tags_enc'])\n",
    "fill_value = -1  # 填充值\n",
    "sequence_length_min = 3  # 最小序列长度\n",
    "sequence_length_max = 10  # 最大序列长度\n",
    "\n",
    "# 填充 'tags_enc'\n",
    "tags_enc_padded = np.array([\n",
    "    np.pad(tags, (0, max_len - len(tags)), 'constant', constant_values=fill_value)\n",
    "    for tags in data['tags_enc']\n",
    "])\n",
    "data['tags_enc_padded'] = list(tags_enc_padded)\n",
    "\n",
    "# 扩展特征列，包含填充后的 'tags_enc_padded'\n",
    "feature_columns_extended = feature_columns.copy()\n",
    "feature_columns_extended.append('tags_enc_padded')\n",
    "\n",
    "# 将特征转换为 NumPy 数组\n",
    "X_numeric = data[feature_columns].values\n",
    "tags_enc_array = np.vstack(data['tags_enc_padded'].values)\n",
    "X_all = np.hstack([X_numeric, tags_enc_array])\n",
    "\n",
    "# 获取标签\n",
    "y_all = data['is_recommended'].values\n",
    "\n",
    "# 过滤掉交互次数少于 3 的用户\n",
    "user_interaction_counts = data['user_id'].value_counts()\n",
    "valid_users = user_interaction_counts[user_interaction_counts >= sequence_length_min].index\n",
    "data_filtered = data[data['user_id'].isin(valid_users)]\n",
    "\n",
    "# 重新构建 X_all 和 y_all，确保只保留过滤后的数据\n",
    "X_numeric = data_filtered[feature_columns].values\n",
    "tags_enc_array = np.vstack(data_filtered['tags_enc_padded'].values)\n",
    "X_all = np.hstack([X_numeric, tags_enc_array])\n",
    "y_all = data_filtered['is_recommended'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/temp\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 105/105 [02:48<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved in ../data/temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义块大小，根据内存情况进行调整\n",
    "chunk_size = 100000\n",
    "\n",
    "# 计算序列总数和块数\n",
    "num_samples = len(data_filtered)\n",
    "num_sequences = num_samples - sequence_length_min + 1\n",
    "num_chunks = (num_sequences + chunk_size - 1) // chunk_size  # 向上取整\n",
    "\n",
    "chunk_counter = 0\n",
    "\n",
    "# 使用 tqdm 显示进度条，遍历每个块\n",
    "for chunk_index in tqdm(range(num_chunks), desc=\"Processing chunks\"):\n",
    "    start_seq = chunk_index * chunk_size\n",
    "    end_seq = min(start_seq + chunk_size, num_sequences)\n",
    "\n",
    "    if end_seq <= start_seq:\n",
    "        continue\n",
    "\n",
    "    X_chunk = X_all[start_seq:end_seq + sequence_length_max - 1]\n",
    "    y_chunk = y_all[start_seq:end_seq + sequence_length_max - 1]\n",
    "\n",
    "    num_chunk_sequences = len(X_chunk) - sequence_length_min + 1\n",
    "    if num_chunk_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    X_chunk_sequences = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_chunk, window_shape=(sequence_length_min, X_chunk.shape[1])\n",
    "    )[:, 0, :, :]\n",
    "\n",
    "    # 如果序列长度不足 10，进行填充\n",
    "    padded_sequences = []\n",
    "    for seq in X_chunk_sequences:\n",
    "        seq_length = len(seq)\n",
    "        if seq_length < sequence_length_max:\n",
    "            padded_seq = np.pad(seq, ((sequence_length_max - seq_length, 0), (0, 0)), 'constant', constant_values=fill_value)\n",
    "            padded_sequences.append(padded_seq)\n",
    "        else:\n",
    "            padded_sequences.append(seq)\n",
    "\n",
    "    X_chunk_sequences_padded = np.array(padded_sequences)\n",
    "    y_chunk_sequences = y_chunk[sequence_length_min - 1:]\n",
    "\n",
    "    # 保存每个块的序列到磁盘\n",
    "    np.save(os.path.join(output_dir, f\"X_chunk_{chunk_counter}.npy\"), X_chunk_sequences_padded)\n",
    "    np.save(os.path.join(output_dir, f\"y_chunk_{chunk_counter}.npy\"), y_chunk_sequences)\n",
    "\n",
    "    chunk_counter += 1\n",
    "\n",
    "print(f\"Data successfully saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved in ../data/processed/sequences_strong/sequences.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# 保存的文件夹路径\n",
    "output_dir = \"../data/temp\"\n",
    "output_file = \"../data/processed/sequences_strong/sequences.h5\"\n",
    "\n",
    "# 确保目标保存目录存在，如果不存在则创建\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# 获取所有分块文件的列表\n",
    "X_files = sorted([f for f in os.listdir(output_dir) if f.startswith(\"X_chunk\")])\n",
    "y_files = sorted([f for f in os.listdir(output_dir) if f.startswith(\"y_chunk\")])\n",
    "\n",
    "# 如果 HDF5 文件已经存在，删除它\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# 初始化 HDF5 文件，并设置数据集的形状\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    first_chunk = np.load(os.path.join(output_dir, X_files[0]))\n",
    "    num_features = first_chunk.shape[2]  # 特征数量\n",
    "    sequence_length = first_chunk.shape[1]  # 序列长度\n",
    "\n",
    "    # 创建数据集（提前分配空间）\n",
    "    X_dset = f.create_dataset(\"X_sequences\", shape=(0, sequence_length, num_features), maxshape=(None, sequence_length, num_features), chunks=True)\n",
    "    y_dset = f.create_dataset(\"y_sequences\", shape=(0,), maxshape=(None,), chunks=True)\n",
    "\n",
    "    # 逐步加载每个块并追加到数据集中\n",
    "    for chunk_counter, (X_file, y_file) in enumerate(zip(X_files, y_files)):\n",
    "        # 加载当前块\n",
    "        X_chunk = np.load(os.path.join(output_dir, X_file))\n",
    "        y_chunk = np.load(os.path.join(output_dir, y_file))\n",
    "\n",
    "        # 更新数据集的大小以适应新数据\n",
    "        X_dset.resize(X_dset.shape[0] + X_chunk.shape[0], axis=0)\n",
    "        y_dset.resize(y_dset.shape[0] + y_chunk.shape[0], axis=0)\n",
    "\n",
    "        # 将新数据写入 HDF5 数据集中\n",
    "        X_dset[-X_chunk.shape[0]:] = X_chunk\n",
    "        y_dset[-y_chunk.shape[0]:] = y_chunk\n",
    "\n",
    "        # 清除当前块的内存\n",
    "        del X_chunk\n",
    "        del y_chunk\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"Data successfully saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full datasets with action padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置序列长度和填充值\n",
    "sequence_length = 3\n",
    "fill_value = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 预处理 'tags_enc' 列，填充为相同的长度\n",
    "max_len = max(len(tags) for tags in data['tags_enc'])\n",
    "tags_enc_padded = np.array([\n",
    "    np.pad(tags, (0, max_len - len(tags)), 'constant', constant_values=fill_value)\n",
    "    for tags in data['tags_enc']\n",
    "])\n",
    "data['tags_enc_padded'] = list(tags_enc_padded)\n",
    "\n",
    "# 扩展特征列，包含填充后的 'tags_enc_padded'\n",
    "feature_columns_extended = feature_columns.copy()\n",
    "feature_columns_extended.append('tags_enc_padded')\n",
    "\n",
    "# 将特征转换为 NumPy 数组\n",
    "X_numeric = data[feature_columns].values\n",
    "tags_enc_array = np.vstack(data['tags_enc_padded'].values)\n",
    "X_all = np.hstack([X_numeric, tags_enc_array])\n",
    "\n",
    "# 获取标签\n",
    "y_all = data['is_recommended'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 170/170 [00:00<00:00, 82014.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义块大小，根据内存情况进行调整\n",
    "chunk_size = 100000\n",
    "\n",
    "# 计算序列总数和块数\n",
    "num_samples = len(data)\n",
    "num_sequences = num_samples - sequence_length + 1\n",
    "num_chunks = (num_sequences + chunk_size - 1) // chunk_size  # 向上取整\n",
    "\n",
    "X_sequences_list = []\n",
    "y_sequences_list = []\n",
    "\n",
    "# 使用 tqdm 显示进度条，遍历每个块\n",
    "for chunk_index in tqdm(range(num_chunks), desc=\"Processing chunks\"):\n",
    "    start_seq = chunk_index * chunk_size\n",
    "    end_seq = min(start_seq + chunk_size, num_sequences)\n",
    "\n",
    "    # 检查是否有足够的数据创建序列\n",
    "    if end_seq <= start_seq:\n",
    "        continue\n",
    "\n",
    "    # 提取当前块的数据\n",
    "    X_chunk = X_all[start_seq:end_seq + sequence_length - 1]\n",
    "    y_chunk = y_all[start_seq:end_seq + sequence_length - 1]\n",
    "\n",
    "    # 使用矢量化操作创建序列\n",
    "    num_chunk_sequences = len(X_chunk) - sequence_length + 1\n",
    "    if num_chunk_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    # 创建特征序列\n",
    "    X_chunk_sequences = np.lib.stride_tricks.sliding_window_view(\n",
    "        X_chunk, window_shape=(sequence_length, X_chunk.shape[1])\n",
    "    )[:, 0, :, :]\n",
    "\n",
    "    # 创建标签序列\n",
    "    y_chunk_sequences = y_chunk[sequence_length - 1:]\n",
    "\n",
    "    # 添加到列表中\n",
    "    X_sequences_list.append(X_chunk_sequences)\n",
    "    y_sequences_list.append(y_chunk_sequences)\n",
    "\n",
    "# 合并所有块的数据\n",
    "X_sequences = np.concatenate(X_sequences_list, axis=0)\n",
    "y_sequences = np.concatenate(y_sequences_list, axis=0)\n",
    "\n",
    "# 检查是否有剩余的数据需要填充\n",
    "remaining_samples = num_samples - (num_sequences + sequence_length - 1)\n",
    "if remaining_samples > 0:\n",
    "    X_padding = np.full((sequence_length - remaining_samples, X_all.shape[1]), fill_value)\n",
    "    X_remaining = np.vstack([X_padding, X_all[-remaining_samples:]])\n",
    "    X_sequences = np.vstack([X_sequences, [X_remaining]])\n",
    "    y_sequences = np.hstack([y_sequences, y_all[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save X_sequences and y_sequences\n",
    "np.save('../data/processed/X_sequences.npy', X_sequences)\n",
    "np.save('../data/processed/y_sequences.npy', y_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "X_sequences = np.load('../data/processed/X_sequences.npy')\n",
    "y_sequences = np.load('../data/processed/y_sequences.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continue for strong partial sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sequences shape: (10414435, 10, 42)\n",
      "y_sequences shape: (10414435,)\n"
     ]
    }
   ],
   "source": [
    "# load the processed strong dataset \n",
    "import h5py\n",
    "\n",
    "h5_file = '../data/processed/sequences_strong/sequences.h5'\n",
    "with h5py.File(h5_file, 'r') as f:\n",
    "    X_sequences = f['X_sequences'][:]\n",
    "    y_sequences = f['y_sequences'][:]\n",
    "\n",
    "print(f\"X_sequences shape: {X_sequences.shape}\")\n",
    "print(f\"y_sequences shape: {y_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_check shape: (16950341, 3, 42)\n",
      "y_check shape: (16950341,)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # load original data\n",
    "# X_check = np.load('../data/processed/sequences_full/X_sequences.npy')\n",
    "# y_check = np.load('../data/processed/sequences_full/y_sequences.npy')\n",
    "\n",
    "# print(f\"X_check shape: {X_check.shape}\")\n",
    "# print(f\"y_check shape: {y_check.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization for timeSeries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展开数据进行标准化\n",
    "num_features = X_sequences.shape[2]\n",
    "X_reshaped = X_sequences.reshape(-1, num_features)\n",
    "# 在读取数据或创建数组时，指定数据类型为 float32\n",
    "X_reshaped = X_reshaped.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normal data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "batch_size = 100000\n",
    "fill_value = -1\n",
    "\n",
    "num_samples = X_reshaped.shape[0]\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size  # 向上取整\n",
    "\n",
    "# 创建一个与 X_reshaped 形状相同的数组，用于存储标准化后的数据\n",
    "# 为了节省内存，我们可以将结果写入文件或逐步处理\n",
    "# 这里我们假设逐步处理，避免一次性创建大数组\n",
    "\n",
    "# 初始化一个列表来存储处理后的批次数据\n",
    "processed_batches = []\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    start_idx = batch_index * batch_size\n",
    "    end_idx = min(start_idx + batch_size, num_samples)\n",
    "    \n",
    "    # 提取当前批次的数据\n",
    "    X_batch = X_reshaped[start_idx:end_idx]\n",
    "    valid_mask = ~(X_batch == fill_value).all(axis=1)\n",
    "    \n",
    "    # 对有效数据进行标准化\n",
    "    X_valid = X_batch[valid_mask]\n",
    "    if X_valid.size > 0:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled_valid = scaler.fit_transform(X_valid)\n",
    "        \n",
    "        # 将标准化后的数据放回原数组的位置\n",
    "        X_batch_scaled = np.full_like(X_batch, fill_value, dtype=np.float32)\n",
    "        X_batch_scaled[valid_mask] = X_scaled_valid.astype(np.float32)\n",
    "    else:\n",
    "        # 如果批次中全是填充值\n",
    "        X_batch_scaled = X_batch.astype(np.float32)\n",
    "    \n",
    "    # 将处理后的批次数据添加到列表中\n",
    "    processed_batches.append(X_batch_scaled)\n",
    "    \n",
    "    print(f\"Processed batch {batch_index + 1}/{num_batches}\")\n",
    "\n",
    "# 将所有批次的数据合并\n",
    "X_scaled = np.concatenate(processed_batches, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For big sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 1/1042\n",
      "Processed and saved batch 2/1042\n",
      "Processed and saved batch 3/1042\n",
      "Processed and saved batch 4/1042\n",
      "Processed and saved batch 5/1042\n",
      "Processed and saved batch 6/1042\n",
      "Processed and saved batch 7/1042\n",
      "Processed and saved batch 8/1042\n",
      "Processed and saved batch 9/1042\n",
      "Processed and saved batch 10/1042\n",
      "Processed and saved batch 11/1042\n",
      "Processed and saved batch 12/1042\n",
      "Processed and saved batch 13/1042\n",
      "Processed and saved batch 14/1042\n",
      "Processed and saved batch 15/1042\n",
      "Processed and saved batch 16/1042\n",
      "Processed and saved batch 17/1042\n",
      "Processed and saved batch 18/1042\n",
      "Processed and saved batch 19/1042\n",
      "Processed and saved batch 20/1042\n",
      "Processed and saved batch 21/1042\n",
      "Processed and saved batch 22/1042\n",
      "Processed and saved batch 23/1042\n",
      "Processed and saved batch 24/1042\n",
      "Processed and saved batch 25/1042\n",
      "Processed and saved batch 26/1042\n",
      "Processed and saved batch 27/1042\n",
      "Processed and saved batch 28/1042\n",
      "Processed and saved batch 29/1042\n",
      "Processed and saved batch 30/1042\n",
      "Processed and saved batch 31/1042\n",
      "Processed and saved batch 32/1042\n",
      "Processed and saved batch 33/1042\n",
      "Processed and saved batch 34/1042\n",
      "Processed and saved batch 35/1042\n",
      "Processed and saved batch 36/1042\n",
      "Processed and saved batch 37/1042\n",
      "Processed and saved batch 38/1042\n",
      "Processed and saved batch 39/1042\n",
      "Processed and saved batch 40/1042\n",
      "Processed and saved batch 41/1042\n",
      "Processed and saved batch 42/1042\n",
      "Processed and saved batch 43/1042\n",
      "Processed and saved batch 44/1042\n",
      "Processed and saved batch 45/1042\n",
      "Processed and saved batch 46/1042\n",
      "Processed and saved batch 47/1042\n",
      "Processed and saved batch 48/1042\n",
      "Processed and saved batch 49/1042\n",
      "Processed and saved batch 50/1042\n",
      "Processed and saved batch 51/1042\n",
      "Processed and saved batch 52/1042\n",
      "Processed and saved batch 53/1042\n",
      "Processed and saved batch 54/1042\n",
      "Processed and saved batch 55/1042\n",
      "Processed and saved batch 56/1042\n",
      "Processed and saved batch 57/1042\n",
      "Processed and saved batch 58/1042\n",
      "Processed and saved batch 59/1042\n",
      "Processed and saved batch 60/1042\n",
      "Processed and saved batch 61/1042\n",
      "Processed and saved batch 62/1042\n",
      "Processed and saved batch 63/1042\n",
      "Processed and saved batch 64/1042\n",
      "Processed and saved batch 65/1042\n",
      "Processed and saved batch 66/1042\n",
      "Processed and saved batch 67/1042\n",
      "Processed and saved batch 68/1042\n",
      "Processed and saved batch 69/1042\n",
      "Processed and saved batch 70/1042\n",
      "Processed and saved batch 71/1042\n",
      "Processed and saved batch 72/1042\n",
      "Processed and saved batch 73/1042\n",
      "Processed and saved batch 74/1042\n",
      "Processed and saved batch 75/1042\n",
      "Processed and saved batch 76/1042\n",
      "Processed and saved batch 77/1042\n",
      "Processed and saved batch 78/1042\n",
      "Processed and saved batch 79/1042\n",
      "Processed and saved batch 80/1042\n",
      "Processed and saved batch 81/1042\n",
      "Processed and saved batch 82/1042\n",
      "Processed and saved batch 83/1042\n",
      "Processed and saved batch 84/1042\n",
      "Processed and saved batch 85/1042\n",
      "Processed and saved batch 86/1042\n",
      "Processed and saved batch 87/1042\n",
      "Processed and saved batch 88/1042\n",
      "Processed and saved batch 89/1042\n",
      "Processed and saved batch 90/1042\n",
      "Processed and saved batch 91/1042\n",
      "Processed and saved batch 92/1042\n",
      "Processed and saved batch 93/1042\n",
      "Processed and saved batch 94/1042\n",
      "Processed and saved batch 95/1042\n",
      "Processed and saved batch 96/1042\n",
      "Processed and saved batch 97/1042\n",
      "Processed and saved batch 98/1042\n",
      "Processed and saved batch 99/1042\n",
      "Processed and saved batch 100/1042\n",
      "Processed and saved batch 101/1042\n",
      "Processed and saved batch 102/1042\n",
      "Processed and saved batch 103/1042\n",
      "Processed and saved batch 104/1042\n",
      "Processed and saved batch 105/1042\n",
      "Processed and saved batch 106/1042\n",
      "Processed and saved batch 107/1042\n",
      "Processed and saved batch 108/1042\n",
      "Processed and saved batch 109/1042\n",
      "Processed and saved batch 110/1042\n",
      "Processed and saved batch 111/1042\n",
      "Processed and saved batch 112/1042\n",
      "Processed and saved batch 113/1042\n",
      "Processed and saved batch 114/1042\n",
      "Processed and saved batch 115/1042\n",
      "Processed and saved batch 116/1042\n",
      "Processed and saved batch 117/1042\n",
      "Processed and saved batch 118/1042\n",
      "Processed and saved batch 119/1042\n",
      "Processed and saved batch 120/1042\n",
      "Processed and saved batch 121/1042\n",
      "Processed and saved batch 122/1042\n",
      "Processed and saved batch 123/1042\n",
      "Processed and saved batch 124/1042\n",
      "Processed and saved batch 125/1042\n",
      "Processed and saved batch 126/1042\n",
      "Processed and saved batch 127/1042\n",
      "Processed and saved batch 128/1042\n",
      "Processed and saved batch 129/1042\n",
      "Processed and saved batch 130/1042\n",
      "Processed and saved batch 131/1042\n",
      "Processed and saved batch 132/1042\n",
      "Processed and saved batch 133/1042\n",
      "Processed and saved batch 134/1042\n",
      "Processed and saved batch 135/1042\n",
      "Processed and saved batch 136/1042\n",
      "Processed and saved batch 137/1042\n",
      "Processed and saved batch 138/1042\n",
      "Processed and saved batch 139/1042\n",
      "Processed and saved batch 140/1042\n",
      "Processed and saved batch 141/1042\n",
      "Processed and saved batch 142/1042\n",
      "Processed and saved batch 143/1042\n",
      "Processed and saved batch 144/1042\n",
      "Processed and saved batch 145/1042\n",
      "Processed and saved batch 146/1042\n",
      "Processed and saved batch 147/1042\n",
      "Processed and saved batch 148/1042\n",
      "Processed and saved batch 149/1042\n",
      "Processed and saved batch 150/1042\n",
      "Processed and saved batch 151/1042\n",
      "Processed and saved batch 152/1042\n",
      "Processed and saved batch 153/1042\n",
      "Processed and saved batch 154/1042\n",
      "Processed and saved batch 155/1042\n",
      "Processed and saved batch 156/1042\n",
      "Processed and saved batch 157/1042\n",
      "Processed and saved batch 158/1042\n",
      "Processed and saved batch 159/1042\n",
      "Processed and saved batch 160/1042\n",
      "Processed and saved batch 161/1042\n",
      "Processed and saved batch 162/1042\n",
      "Processed and saved batch 163/1042\n",
      "Processed and saved batch 164/1042\n",
      "Processed and saved batch 165/1042\n",
      "Processed and saved batch 166/1042\n",
      "Processed and saved batch 167/1042\n",
      "Processed and saved batch 168/1042\n",
      "Processed and saved batch 169/1042\n",
      "Processed and saved batch 170/1042\n",
      "Processed and saved batch 171/1042\n",
      "Processed and saved batch 172/1042\n",
      "Processed and saved batch 173/1042\n",
      "Processed and saved batch 174/1042\n",
      "Processed and saved batch 175/1042\n",
      "Processed and saved batch 176/1042\n",
      "Processed and saved batch 177/1042\n",
      "Processed and saved batch 178/1042\n",
      "Processed and saved batch 179/1042\n",
      "Processed and saved batch 180/1042\n",
      "Processed and saved batch 181/1042\n",
      "Processed and saved batch 182/1042\n",
      "Processed and saved batch 183/1042\n",
      "Processed and saved batch 184/1042\n",
      "Processed and saved batch 185/1042\n",
      "Processed and saved batch 186/1042\n",
      "Processed and saved batch 187/1042\n",
      "Processed and saved batch 188/1042\n",
      "Processed and saved batch 189/1042\n",
      "Processed and saved batch 190/1042\n",
      "Processed and saved batch 191/1042\n",
      "Processed and saved batch 192/1042\n",
      "Processed and saved batch 193/1042\n",
      "Processed and saved batch 194/1042\n",
      "Processed and saved batch 195/1042\n",
      "Processed and saved batch 196/1042\n",
      "Processed and saved batch 197/1042\n",
      "Processed and saved batch 198/1042\n",
      "Processed and saved batch 199/1042\n",
      "Processed and saved batch 200/1042\n",
      "Processed and saved batch 201/1042\n",
      "Processed and saved batch 202/1042\n",
      "Processed and saved batch 203/1042\n",
      "Processed and saved batch 204/1042\n",
      "Processed and saved batch 205/1042\n",
      "Processed and saved batch 206/1042\n",
      "Processed and saved batch 207/1042\n",
      "Processed and saved batch 208/1042\n",
      "Processed and saved batch 209/1042\n",
      "Processed and saved batch 210/1042\n",
      "Processed and saved batch 211/1042\n",
      "Processed and saved batch 212/1042\n",
      "Processed and saved batch 213/1042\n",
      "Processed and saved batch 214/1042\n",
      "Processed and saved batch 215/1042\n",
      "Processed and saved batch 216/1042\n",
      "Processed and saved batch 217/1042\n",
      "Processed and saved batch 218/1042\n",
      "Processed and saved batch 219/1042\n",
      "Processed and saved batch 220/1042\n",
      "Processed and saved batch 221/1042\n",
      "Processed and saved batch 222/1042\n",
      "Processed and saved batch 223/1042\n",
      "Processed and saved batch 224/1042\n",
      "Processed and saved batch 225/1042\n",
      "Processed and saved batch 226/1042\n",
      "Processed and saved batch 227/1042\n",
      "Processed and saved batch 228/1042\n",
      "Processed and saved batch 229/1042\n",
      "Processed and saved batch 230/1042\n",
      "Processed and saved batch 231/1042\n",
      "Processed and saved batch 232/1042\n",
      "Processed and saved batch 233/1042\n",
      "Processed and saved batch 234/1042\n",
      "Processed and saved batch 235/1042\n",
      "Processed and saved batch 236/1042\n",
      "Processed and saved batch 237/1042\n",
      "Processed and saved batch 238/1042\n",
      "Processed and saved batch 239/1042\n",
      "Processed and saved batch 240/1042\n",
      "Processed and saved batch 241/1042\n",
      "Processed and saved batch 242/1042\n",
      "Processed and saved batch 243/1042\n",
      "Processed and saved batch 244/1042\n",
      "Processed and saved batch 245/1042\n",
      "Processed and saved batch 246/1042\n",
      "Processed and saved batch 247/1042\n",
      "Processed and saved batch 248/1042\n",
      "Processed and saved batch 249/1042\n",
      "Processed and saved batch 250/1042\n",
      "Processed and saved batch 251/1042\n",
      "Processed and saved batch 252/1042\n",
      "Processed and saved batch 253/1042\n",
      "Processed and saved batch 254/1042\n",
      "Processed and saved batch 255/1042\n",
      "Processed and saved batch 256/1042\n",
      "Processed and saved batch 257/1042\n",
      "Processed and saved batch 258/1042\n",
      "Processed and saved batch 259/1042\n",
      "Processed and saved batch 260/1042\n",
      "Processed and saved batch 261/1042\n",
      "Processed and saved batch 262/1042\n",
      "Processed and saved batch 263/1042\n",
      "Processed and saved batch 264/1042\n",
      "Processed and saved batch 265/1042\n",
      "Processed and saved batch 266/1042\n",
      "Processed and saved batch 267/1042\n",
      "Processed and saved batch 268/1042\n",
      "Processed and saved batch 269/1042\n",
      "Processed and saved batch 270/1042\n",
      "Processed and saved batch 271/1042\n",
      "Processed and saved batch 272/1042\n",
      "Processed and saved batch 273/1042\n",
      "Processed and saved batch 274/1042\n",
      "Processed and saved batch 275/1042\n",
      "Processed and saved batch 276/1042\n",
      "Processed and saved batch 277/1042\n",
      "Processed and saved batch 278/1042\n",
      "Processed and saved batch 279/1042\n",
      "Processed and saved batch 280/1042\n",
      "Processed and saved batch 281/1042\n",
      "Processed and saved batch 282/1042\n",
      "Processed and saved batch 283/1042\n",
      "Processed and saved batch 284/1042\n",
      "Processed and saved batch 285/1042\n",
      "Processed and saved batch 286/1042\n",
      "Processed and saved batch 287/1042\n",
      "Processed and saved batch 288/1042\n",
      "Processed and saved batch 289/1042\n",
      "Processed and saved batch 290/1042\n",
      "Processed and saved batch 291/1042\n",
      "Processed and saved batch 292/1042\n",
      "Processed and saved batch 293/1042\n",
      "Processed and saved batch 294/1042\n",
      "Processed and saved batch 295/1042\n",
      "Processed and saved batch 296/1042\n",
      "Processed and saved batch 297/1042\n",
      "Processed and saved batch 298/1042\n",
      "Processed and saved batch 299/1042\n",
      "Processed and saved batch 300/1042\n",
      "Processed and saved batch 301/1042\n",
      "Processed and saved batch 302/1042\n",
      "Processed and saved batch 303/1042\n",
      "Processed and saved batch 304/1042\n",
      "Processed and saved batch 305/1042\n",
      "Processed and saved batch 306/1042\n",
      "Processed and saved batch 307/1042\n",
      "Processed and saved batch 308/1042\n",
      "Processed and saved batch 309/1042\n",
      "Processed and saved batch 310/1042\n",
      "Processed and saved batch 311/1042\n",
      "Processed and saved batch 312/1042\n",
      "Processed and saved batch 313/1042\n",
      "Processed and saved batch 314/1042\n",
      "Processed and saved batch 315/1042\n",
      "Processed and saved batch 316/1042\n",
      "Processed and saved batch 317/1042\n",
      "Processed and saved batch 318/1042\n",
      "Processed and saved batch 319/1042\n",
      "Processed and saved batch 320/1042\n",
      "Processed and saved batch 321/1042\n",
      "Processed and saved batch 322/1042\n",
      "Processed and saved batch 323/1042\n",
      "Processed and saved batch 324/1042\n",
      "Processed and saved batch 325/1042\n",
      "Processed and saved batch 326/1042\n",
      "Processed and saved batch 327/1042\n",
      "Processed and saved batch 328/1042\n",
      "Processed and saved batch 329/1042\n",
      "Processed and saved batch 330/1042\n",
      "Processed and saved batch 331/1042\n",
      "Processed and saved batch 332/1042\n",
      "Processed and saved batch 333/1042\n",
      "Processed and saved batch 334/1042\n",
      "Processed and saved batch 335/1042\n",
      "Processed and saved batch 336/1042\n",
      "Processed and saved batch 337/1042\n",
      "Processed and saved batch 338/1042\n",
      "Processed and saved batch 339/1042\n",
      "Processed and saved batch 340/1042\n",
      "Processed and saved batch 341/1042\n",
      "Processed and saved batch 342/1042\n",
      "Processed and saved batch 343/1042\n",
      "Processed and saved batch 344/1042\n",
      "Processed and saved batch 345/1042\n",
      "Processed and saved batch 346/1042\n",
      "Processed and saved batch 347/1042\n",
      "Processed and saved batch 348/1042\n",
      "Processed and saved batch 349/1042\n",
      "Processed and saved batch 350/1042\n",
      "Processed and saved batch 351/1042\n",
      "Processed and saved batch 352/1042\n",
      "Processed and saved batch 353/1042\n",
      "Processed and saved batch 354/1042\n",
      "Processed and saved batch 355/1042\n",
      "Processed and saved batch 356/1042\n",
      "Processed and saved batch 357/1042\n",
      "Processed and saved batch 358/1042\n",
      "Processed and saved batch 359/1042\n",
      "Processed and saved batch 360/1042\n",
      "Processed and saved batch 361/1042\n",
      "Processed and saved batch 362/1042\n",
      "Processed and saved batch 363/1042\n",
      "Processed and saved batch 364/1042\n",
      "Processed and saved batch 365/1042\n",
      "Processed and saved batch 366/1042\n",
      "Processed and saved batch 367/1042\n",
      "Processed and saved batch 368/1042\n",
      "Processed and saved batch 369/1042\n",
      "Processed and saved batch 370/1042\n",
      "Processed and saved batch 371/1042\n",
      "Processed and saved batch 372/1042\n",
      "Processed and saved batch 373/1042\n",
      "Processed and saved batch 374/1042\n",
      "Processed and saved batch 375/1042\n",
      "Processed and saved batch 376/1042\n",
      "Processed and saved batch 377/1042\n",
      "Processed and saved batch 378/1042\n",
      "Processed and saved batch 379/1042\n",
      "Processed and saved batch 380/1042\n",
      "Processed and saved batch 381/1042\n",
      "Processed and saved batch 382/1042\n",
      "Processed and saved batch 383/1042\n",
      "Processed and saved batch 384/1042\n",
      "Processed and saved batch 385/1042\n",
      "Processed and saved batch 386/1042\n",
      "Processed and saved batch 387/1042\n",
      "Processed and saved batch 388/1042\n",
      "Processed and saved batch 389/1042\n",
      "Processed and saved batch 390/1042\n",
      "Processed and saved batch 391/1042\n",
      "Processed and saved batch 392/1042\n",
      "Processed and saved batch 393/1042\n",
      "Processed and saved batch 394/1042\n",
      "Processed and saved batch 395/1042\n",
      "Processed and saved batch 396/1042\n",
      "Processed and saved batch 397/1042\n",
      "Processed and saved batch 398/1042\n",
      "Processed and saved batch 399/1042\n",
      "Processed and saved batch 400/1042\n",
      "Processed and saved batch 401/1042\n",
      "Processed and saved batch 402/1042\n",
      "Processed and saved batch 403/1042\n",
      "Processed and saved batch 404/1042\n",
      "Processed and saved batch 405/1042\n",
      "Processed and saved batch 406/1042\n",
      "Processed and saved batch 407/1042\n",
      "Processed and saved batch 408/1042\n",
      "Processed and saved batch 409/1042\n",
      "Processed and saved batch 410/1042\n",
      "Processed and saved batch 411/1042\n",
      "Processed and saved batch 412/1042\n",
      "Processed and saved batch 413/1042\n",
      "Processed and saved batch 414/1042\n",
      "Processed and saved batch 415/1042\n",
      "Processed and saved batch 416/1042\n",
      "Processed and saved batch 417/1042\n",
      "Processed and saved batch 418/1042\n",
      "Processed and saved batch 419/1042\n",
      "Processed and saved batch 420/1042\n",
      "Processed and saved batch 421/1042\n",
      "Processed and saved batch 422/1042\n",
      "Processed and saved batch 423/1042\n",
      "Processed and saved batch 424/1042\n",
      "Processed and saved batch 425/1042\n",
      "Processed and saved batch 426/1042\n",
      "Processed and saved batch 427/1042\n",
      "Processed and saved batch 428/1042\n",
      "Processed and saved batch 429/1042\n",
      "Processed and saved batch 430/1042\n",
      "Processed and saved batch 431/1042\n",
      "Processed and saved batch 432/1042\n",
      "Processed and saved batch 433/1042\n",
      "Processed and saved batch 434/1042\n",
      "Processed and saved batch 435/1042\n",
      "Processed and saved batch 436/1042\n",
      "Processed and saved batch 437/1042\n",
      "Processed and saved batch 438/1042\n",
      "Processed and saved batch 439/1042\n",
      "Processed and saved batch 440/1042\n",
      "Processed and saved batch 441/1042\n",
      "Processed and saved batch 442/1042\n",
      "Processed and saved batch 443/1042\n",
      "Processed and saved batch 444/1042\n",
      "Processed and saved batch 445/1042\n",
      "Processed and saved batch 446/1042\n",
      "Processed and saved batch 447/1042\n",
      "Processed and saved batch 448/1042\n",
      "Processed and saved batch 449/1042\n",
      "Processed and saved batch 450/1042\n",
      "Processed and saved batch 451/1042\n",
      "Processed and saved batch 452/1042\n",
      "Processed and saved batch 453/1042\n",
      "Processed and saved batch 454/1042\n",
      "Processed and saved batch 455/1042\n",
      "Processed and saved batch 456/1042\n",
      "Processed and saved batch 457/1042\n",
      "Processed and saved batch 458/1042\n",
      "Processed and saved batch 459/1042\n",
      "Processed and saved batch 460/1042\n",
      "Processed and saved batch 461/1042\n",
      "Processed and saved batch 462/1042\n",
      "Processed and saved batch 463/1042\n",
      "Processed and saved batch 464/1042\n",
      "Processed and saved batch 465/1042\n",
      "Processed and saved batch 466/1042\n",
      "Processed and saved batch 467/1042\n",
      "Processed and saved batch 468/1042\n",
      "Processed and saved batch 469/1042\n",
      "Processed and saved batch 470/1042\n",
      "Processed and saved batch 471/1042\n",
      "Processed and saved batch 472/1042\n",
      "Processed and saved batch 473/1042\n",
      "Processed and saved batch 474/1042\n",
      "Processed and saved batch 475/1042\n",
      "Processed and saved batch 476/1042\n",
      "Processed and saved batch 477/1042\n",
      "Processed and saved batch 478/1042\n",
      "Processed and saved batch 479/1042\n",
      "Processed and saved batch 480/1042\n",
      "Processed and saved batch 481/1042\n",
      "Processed and saved batch 482/1042\n",
      "Processed and saved batch 483/1042\n",
      "Processed and saved batch 484/1042\n",
      "Processed and saved batch 485/1042\n",
      "Processed and saved batch 486/1042\n",
      "Processed and saved batch 487/1042\n",
      "Processed and saved batch 488/1042\n",
      "Processed and saved batch 489/1042\n",
      "Processed and saved batch 490/1042\n",
      "Processed and saved batch 491/1042\n",
      "Processed and saved batch 492/1042\n",
      "Processed and saved batch 493/1042\n",
      "Processed and saved batch 494/1042\n",
      "Processed and saved batch 495/1042\n",
      "Processed and saved batch 496/1042\n",
      "Processed and saved batch 497/1042\n",
      "Processed and saved batch 498/1042\n",
      "Processed and saved batch 499/1042\n",
      "Processed and saved batch 500/1042\n",
      "Processed and saved batch 501/1042\n",
      "Processed and saved batch 502/1042\n",
      "Processed and saved batch 503/1042\n",
      "Processed and saved batch 504/1042\n",
      "Processed and saved batch 505/1042\n",
      "Processed and saved batch 506/1042\n",
      "Processed and saved batch 507/1042\n",
      "Processed and saved batch 508/1042\n",
      "Processed and saved batch 509/1042\n",
      "Processed and saved batch 510/1042\n",
      "Processed and saved batch 511/1042\n",
      "Processed and saved batch 512/1042\n",
      "Processed and saved batch 513/1042\n",
      "Processed and saved batch 514/1042\n",
      "Processed and saved batch 515/1042\n",
      "Processed and saved batch 516/1042\n",
      "Processed and saved batch 517/1042\n",
      "Processed and saved batch 518/1042\n",
      "Processed and saved batch 519/1042\n",
      "Processed and saved batch 520/1042\n",
      "Processed and saved batch 521/1042\n",
      "Processed and saved batch 522/1042\n",
      "Processed and saved batch 523/1042\n",
      "Processed and saved batch 524/1042\n",
      "Processed and saved batch 525/1042\n",
      "Processed and saved batch 526/1042\n",
      "Processed and saved batch 527/1042\n",
      "Processed and saved batch 528/1042\n",
      "Processed and saved batch 529/1042\n",
      "Processed and saved batch 530/1042\n",
      "Processed and saved batch 531/1042\n",
      "Processed and saved batch 532/1042\n",
      "Processed and saved batch 533/1042\n",
      "Processed and saved batch 534/1042\n",
      "Processed and saved batch 535/1042\n",
      "Processed and saved batch 536/1042\n",
      "Processed and saved batch 537/1042\n",
      "Processed and saved batch 538/1042\n",
      "Processed and saved batch 539/1042\n",
      "Processed and saved batch 540/1042\n",
      "Processed and saved batch 541/1042\n",
      "Processed and saved batch 542/1042\n",
      "Processed and saved batch 543/1042\n",
      "Processed and saved batch 544/1042\n",
      "Processed and saved batch 545/1042\n",
      "Processed and saved batch 546/1042\n",
      "Processed and saved batch 547/1042\n",
      "Processed and saved batch 548/1042\n",
      "Processed and saved batch 549/1042\n",
      "Processed and saved batch 550/1042\n",
      "Processed and saved batch 551/1042\n",
      "Processed and saved batch 552/1042\n",
      "Processed and saved batch 553/1042\n",
      "Processed and saved batch 554/1042\n",
      "Processed and saved batch 555/1042\n",
      "Processed and saved batch 556/1042\n",
      "Processed and saved batch 557/1042\n",
      "Processed and saved batch 558/1042\n",
      "Processed and saved batch 559/1042\n",
      "Processed and saved batch 560/1042\n",
      "Processed and saved batch 561/1042\n",
      "Processed and saved batch 562/1042\n",
      "Processed and saved batch 563/1042\n",
      "Processed and saved batch 564/1042\n",
      "Processed and saved batch 565/1042\n",
      "Processed and saved batch 566/1042\n",
      "Processed and saved batch 567/1042\n",
      "Processed and saved batch 568/1042\n",
      "Processed and saved batch 569/1042\n",
      "Processed and saved batch 570/1042\n",
      "Processed and saved batch 571/1042\n",
      "Processed and saved batch 572/1042\n",
      "Processed and saved batch 573/1042\n",
      "Processed and saved batch 574/1042\n",
      "Processed and saved batch 575/1042\n",
      "Processed and saved batch 576/1042\n",
      "Processed and saved batch 577/1042\n",
      "Processed and saved batch 578/1042\n",
      "Processed and saved batch 579/1042\n",
      "Processed and saved batch 580/1042\n",
      "Processed and saved batch 581/1042\n",
      "Processed and saved batch 582/1042\n",
      "Processed and saved batch 583/1042\n",
      "Processed and saved batch 584/1042\n",
      "Processed and saved batch 585/1042\n",
      "Processed and saved batch 586/1042\n",
      "Processed and saved batch 587/1042\n",
      "Processed and saved batch 588/1042\n",
      "Processed and saved batch 589/1042\n",
      "Processed and saved batch 590/1042\n",
      "Processed and saved batch 591/1042\n",
      "Processed and saved batch 592/1042\n",
      "Processed and saved batch 593/1042\n",
      "Processed and saved batch 594/1042\n",
      "Processed and saved batch 595/1042\n",
      "Processed and saved batch 596/1042\n",
      "Processed and saved batch 597/1042\n",
      "Processed and saved batch 598/1042\n",
      "Processed and saved batch 599/1042\n",
      "Processed and saved batch 600/1042\n",
      "Processed and saved batch 601/1042\n",
      "Processed and saved batch 602/1042\n",
      "Processed and saved batch 603/1042\n",
      "Processed and saved batch 604/1042\n",
      "Processed and saved batch 605/1042\n",
      "Processed and saved batch 606/1042\n",
      "Processed and saved batch 607/1042\n",
      "Processed and saved batch 608/1042\n",
      "Processed and saved batch 609/1042\n",
      "Processed and saved batch 610/1042\n",
      "Processed and saved batch 611/1042\n",
      "Processed and saved batch 612/1042\n",
      "Processed and saved batch 613/1042\n",
      "Processed and saved batch 614/1042\n",
      "Processed and saved batch 615/1042\n",
      "Processed and saved batch 616/1042\n",
      "Processed and saved batch 617/1042\n",
      "Processed and saved batch 618/1042\n",
      "Processed and saved batch 619/1042\n",
      "Processed and saved batch 620/1042\n",
      "Processed and saved batch 621/1042\n",
      "Processed and saved batch 622/1042\n",
      "Processed and saved batch 623/1042\n",
      "Processed and saved batch 624/1042\n",
      "Processed and saved batch 625/1042\n",
      "Processed and saved batch 626/1042\n",
      "Processed and saved batch 627/1042\n",
      "Processed and saved batch 628/1042\n",
      "Processed and saved batch 629/1042\n",
      "Processed and saved batch 630/1042\n",
      "Processed and saved batch 631/1042\n",
      "Processed and saved batch 632/1042\n",
      "Processed and saved batch 633/1042\n",
      "Processed and saved batch 634/1042\n",
      "Processed and saved batch 635/1042\n",
      "Processed and saved batch 636/1042\n",
      "Processed and saved batch 637/1042\n",
      "Processed and saved batch 638/1042\n",
      "Processed and saved batch 639/1042\n",
      "Processed and saved batch 640/1042\n",
      "Processed and saved batch 641/1042\n",
      "Processed and saved batch 642/1042\n",
      "Processed and saved batch 643/1042\n",
      "Processed and saved batch 644/1042\n",
      "Processed and saved batch 645/1042\n",
      "Processed and saved batch 646/1042\n",
      "Processed and saved batch 647/1042\n",
      "Processed and saved batch 648/1042\n",
      "Processed and saved batch 649/1042\n",
      "Processed and saved batch 650/1042\n",
      "Processed and saved batch 651/1042\n",
      "Processed and saved batch 652/1042\n",
      "Processed and saved batch 653/1042\n",
      "Processed and saved batch 654/1042\n",
      "Processed and saved batch 655/1042\n",
      "Processed and saved batch 656/1042\n",
      "Processed and saved batch 657/1042\n",
      "Processed and saved batch 658/1042\n",
      "Processed and saved batch 659/1042\n",
      "Processed and saved batch 660/1042\n",
      "Processed and saved batch 661/1042\n",
      "Processed and saved batch 662/1042\n",
      "Processed and saved batch 663/1042\n",
      "Processed and saved batch 664/1042\n",
      "Processed and saved batch 665/1042\n",
      "Processed and saved batch 666/1042\n",
      "Processed and saved batch 667/1042\n",
      "Processed and saved batch 668/1042\n",
      "Processed and saved batch 669/1042\n",
      "Processed and saved batch 670/1042\n",
      "Processed and saved batch 671/1042\n",
      "Processed and saved batch 672/1042\n",
      "Processed and saved batch 673/1042\n",
      "Processed and saved batch 674/1042\n",
      "Processed and saved batch 675/1042\n",
      "Processed and saved batch 676/1042\n",
      "Processed and saved batch 677/1042\n",
      "Processed and saved batch 678/1042\n",
      "Processed and saved batch 679/1042\n",
      "Processed and saved batch 680/1042\n",
      "Processed and saved batch 681/1042\n",
      "Processed and saved batch 682/1042\n",
      "Processed and saved batch 683/1042\n",
      "Processed and saved batch 684/1042\n",
      "Processed and saved batch 685/1042\n",
      "Processed and saved batch 686/1042\n",
      "Processed and saved batch 687/1042\n",
      "Processed and saved batch 688/1042\n",
      "Processed and saved batch 689/1042\n",
      "Processed and saved batch 690/1042\n",
      "Processed and saved batch 691/1042\n",
      "Processed and saved batch 692/1042\n",
      "Processed and saved batch 693/1042\n",
      "Processed and saved batch 694/1042\n",
      "Processed and saved batch 695/1042\n",
      "Processed and saved batch 696/1042\n",
      "Processed and saved batch 697/1042\n",
      "Processed and saved batch 698/1042\n",
      "Processed and saved batch 699/1042\n",
      "Processed and saved batch 700/1042\n",
      "Processed and saved batch 701/1042\n",
      "Processed and saved batch 702/1042\n",
      "Processed and saved batch 703/1042\n",
      "Processed and saved batch 704/1042\n",
      "Processed and saved batch 705/1042\n",
      "Processed and saved batch 706/1042\n",
      "Processed and saved batch 707/1042\n",
      "Processed and saved batch 708/1042\n",
      "Processed and saved batch 709/1042\n",
      "Processed and saved batch 710/1042\n",
      "Processed and saved batch 711/1042\n",
      "Processed and saved batch 712/1042\n",
      "Processed and saved batch 713/1042\n",
      "Processed and saved batch 714/1042\n",
      "Processed and saved batch 715/1042\n",
      "Processed and saved batch 716/1042\n",
      "Processed and saved batch 717/1042\n",
      "Processed and saved batch 718/1042\n",
      "Processed and saved batch 719/1042\n",
      "Processed and saved batch 720/1042\n",
      "Processed and saved batch 721/1042\n",
      "Processed and saved batch 722/1042\n",
      "Processed and saved batch 723/1042\n",
      "Processed and saved batch 724/1042\n",
      "Processed and saved batch 725/1042\n",
      "Processed and saved batch 726/1042\n",
      "Processed and saved batch 727/1042\n",
      "Processed and saved batch 728/1042\n",
      "Processed and saved batch 729/1042\n",
      "Processed and saved batch 730/1042\n",
      "Processed and saved batch 731/1042\n",
      "Processed and saved batch 732/1042\n",
      "Processed and saved batch 733/1042\n",
      "Processed and saved batch 734/1042\n",
      "Processed and saved batch 735/1042\n",
      "Processed and saved batch 736/1042\n",
      "Processed and saved batch 737/1042\n",
      "Processed and saved batch 738/1042\n",
      "Processed and saved batch 739/1042\n",
      "Processed and saved batch 740/1042\n",
      "Processed and saved batch 741/1042\n",
      "Processed and saved batch 742/1042\n",
      "Processed and saved batch 743/1042\n",
      "Processed and saved batch 744/1042\n",
      "Processed and saved batch 745/1042\n",
      "Processed and saved batch 746/1042\n",
      "Processed and saved batch 747/1042\n",
      "Processed and saved batch 748/1042\n",
      "Processed and saved batch 749/1042\n",
      "Processed and saved batch 750/1042\n",
      "Processed and saved batch 751/1042\n",
      "Processed and saved batch 752/1042\n",
      "Processed and saved batch 753/1042\n",
      "Processed and saved batch 754/1042\n",
      "Processed and saved batch 755/1042\n",
      "Processed and saved batch 756/1042\n",
      "Processed and saved batch 757/1042\n",
      "Processed and saved batch 758/1042\n",
      "Processed and saved batch 759/1042\n",
      "Processed and saved batch 760/1042\n",
      "Processed and saved batch 761/1042\n",
      "Processed and saved batch 762/1042\n",
      "Processed and saved batch 763/1042\n",
      "Processed and saved batch 764/1042\n",
      "Processed and saved batch 765/1042\n",
      "Processed and saved batch 766/1042\n",
      "Processed and saved batch 767/1042\n",
      "Processed and saved batch 768/1042\n",
      "Processed and saved batch 769/1042\n",
      "Processed and saved batch 770/1042\n",
      "Processed and saved batch 771/1042\n",
      "Processed and saved batch 772/1042\n",
      "Processed and saved batch 773/1042\n",
      "Processed and saved batch 774/1042\n",
      "Processed and saved batch 775/1042\n",
      "Processed and saved batch 776/1042\n",
      "Processed and saved batch 777/1042\n",
      "Processed and saved batch 778/1042\n",
      "Processed and saved batch 779/1042\n",
      "Processed and saved batch 780/1042\n",
      "Processed and saved batch 781/1042\n",
      "Processed and saved batch 782/1042\n",
      "Processed and saved batch 783/1042\n",
      "Processed and saved batch 784/1042\n",
      "Processed and saved batch 785/1042\n",
      "Processed and saved batch 786/1042\n",
      "Processed and saved batch 787/1042\n",
      "Processed and saved batch 788/1042\n",
      "Processed and saved batch 789/1042\n",
      "Processed and saved batch 790/1042\n",
      "Processed and saved batch 791/1042\n",
      "Processed and saved batch 792/1042\n",
      "Processed and saved batch 793/1042\n",
      "Processed and saved batch 794/1042\n",
      "Processed and saved batch 795/1042\n",
      "Processed and saved batch 796/1042\n",
      "Processed and saved batch 797/1042\n",
      "Processed and saved batch 798/1042\n",
      "Processed and saved batch 799/1042\n",
      "Processed and saved batch 800/1042\n",
      "Processed and saved batch 801/1042\n",
      "Processed and saved batch 802/1042\n",
      "Processed and saved batch 803/1042\n",
      "Processed and saved batch 804/1042\n",
      "Processed and saved batch 805/1042\n",
      "Processed and saved batch 806/1042\n",
      "Processed and saved batch 807/1042\n",
      "Processed and saved batch 808/1042\n",
      "Processed and saved batch 809/1042\n",
      "Processed and saved batch 810/1042\n",
      "Processed and saved batch 811/1042\n",
      "Processed and saved batch 812/1042\n",
      "Processed and saved batch 813/1042\n",
      "Processed and saved batch 814/1042\n",
      "Processed and saved batch 815/1042\n",
      "Processed and saved batch 816/1042\n",
      "Processed and saved batch 817/1042\n",
      "Processed and saved batch 818/1042\n",
      "Processed and saved batch 819/1042\n",
      "Processed and saved batch 820/1042\n",
      "Processed and saved batch 821/1042\n",
      "Processed and saved batch 822/1042\n",
      "Processed and saved batch 823/1042\n",
      "Processed and saved batch 824/1042\n",
      "Processed and saved batch 825/1042\n",
      "Processed and saved batch 826/1042\n",
      "Processed and saved batch 827/1042\n",
      "Processed and saved batch 828/1042\n",
      "Processed and saved batch 829/1042\n",
      "Processed and saved batch 830/1042\n",
      "Processed and saved batch 831/1042\n",
      "Processed and saved batch 832/1042\n",
      "Processed and saved batch 833/1042\n",
      "Processed and saved batch 834/1042\n",
      "Processed and saved batch 835/1042\n",
      "Processed and saved batch 836/1042\n",
      "Processed and saved batch 837/1042\n",
      "Processed and saved batch 838/1042\n",
      "Processed and saved batch 839/1042\n",
      "Processed and saved batch 840/1042\n",
      "Processed and saved batch 841/1042\n",
      "Processed and saved batch 842/1042\n",
      "Processed and saved batch 843/1042\n",
      "Processed and saved batch 844/1042\n",
      "Processed and saved batch 845/1042\n",
      "Processed and saved batch 846/1042\n",
      "Processed and saved batch 847/1042\n",
      "Processed and saved batch 848/1042\n",
      "Processed and saved batch 849/1042\n",
      "Processed and saved batch 850/1042\n",
      "Processed and saved batch 851/1042\n",
      "Processed and saved batch 852/1042\n",
      "Processed and saved batch 853/1042\n",
      "Processed and saved batch 854/1042\n",
      "Processed and saved batch 855/1042\n",
      "Processed and saved batch 856/1042\n",
      "Processed and saved batch 857/1042\n",
      "Processed and saved batch 858/1042\n",
      "Processed and saved batch 859/1042\n",
      "Processed and saved batch 860/1042\n",
      "Processed and saved batch 861/1042\n",
      "Processed and saved batch 862/1042\n",
      "Processed and saved batch 863/1042\n",
      "Processed and saved batch 864/1042\n",
      "Processed and saved batch 865/1042\n",
      "Processed and saved batch 866/1042\n",
      "Processed and saved batch 867/1042\n",
      "Processed and saved batch 868/1042\n",
      "Processed and saved batch 869/1042\n",
      "Processed and saved batch 870/1042\n",
      "Processed and saved batch 871/1042\n",
      "Processed and saved batch 872/1042\n",
      "Processed and saved batch 873/1042\n",
      "Processed and saved batch 874/1042\n",
      "Processed and saved batch 875/1042\n",
      "Processed and saved batch 876/1042\n",
      "Processed and saved batch 877/1042\n",
      "Processed and saved batch 878/1042\n",
      "Processed and saved batch 879/1042\n",
      "Processed and saved batch 880/1042\n",
      "Processed and saved batch 881/1042\n",
      "Processed and saved batch 882/1042\n",
      "Processed and saved batch 883/1042\n",
      "Processed and saved batch 884/1042\n",
      "Processed and saved batch 885/1042\n",
      "Processed and saved batch 886/1042\n",
      "Processed and saved batch 887/1042\n",
      "Processed and saved batch 888/1042\n",
      "Processed and saved batch 889/1042\n",
      "Processed and saved batch 890/1042\n",
      "Processed and saved batch 891/1042\n",
      "Processed and saved batch 892/1042\n",
      "Processed and saved batch 893/1042\n",
      "Processed and saved batch 894/1042\n",
      "Processed and saved batch 895/1042\n",
      "Processed and saved batch 896/1042\n",
      "Processed and saved batch 897/1042\n",
      "Processed and saved batch 898/1042\n",
      "Processed and saved batch 899/1042\n",
      "Processed and saved batch 900/1042\n",
      "Processed and saved batch 901/1042\n",
      "Processed and saved batch 902/1042\n",
      "Processed and saved batch 903/1042\n",
      "Processed and saved batch 904/1042\n",
      "Processed and saved batch 905/1042\n",
      "Processed and saved batch 906/1042\n",
      "Processed and saved batch 907/1042\n",
      "Processed and saved batch 908/1042\n",
      "Processed and saved batch 909/1042\n",
      "Processed and saved batch 910/1042\n",
      "Processed and saved batch 911/1042\n",
      "Processed and saved batch 912/1042\n",
      "Processed and saved batch 913/1042\n",
      "Processed and saved batch 914/1042\n",
      "Processed and saved batch 915/1042\n",
      "Processed and saved batch 916/1042\n",
      "Processed and saved batch 917/1042\n",
      "Processed and saved batch 918/1042\n",
      "Processed and saved batch 919/1042\n",
      "Processed and saved batch 920/1042\n",
      "Processed and saved batch 921/1042\n",
      "Processed and saved batch 922/1042\n",
      "Processed and saved batch 923/1042\n",
      "Processed and saved batch 924/1042\n",
      "Processed and saved batch 925/1042\n",
      "Processed and saved batch 926/1042\n",
      "Processed and saved batch 927/1042\n",
      "Processed and saved batch 928/1042\n",
      "Processed and saved batch 929/1042\n",
      "Processed and saved batch 930/1042\n",
      "Processed and saved batch 931/1042\n",
      "Processed and saved batch 932/1042\n",
      "Processed and saved batch 933/1042\n",
      "Processed and saved batch 934/1042\n",
      "Processed and saved batch 935/1042\n",
      "Processed and saved batch 936/1042\n",
      "Processed and saved batch 937/1042\n",
      "Processed and saved batch 938/1042\n",
      "Processed and saved batch 939/1042\n",
      "Processed and saved batch 940/1042\n",
      "Processed and saved batch 941/1042\n",
      "Processed and saved batch 942/1042\n",
      "Processed and saved batch 943/1042\n",
      "Processed and saved batch 944/1042\n",
      "Processed and saved batch 945/1042\n",
      "Processed and saved batch 946/1042\n",
      "Processed and saved batch 947/1042\n",
      "Processed and saved batch 948/1042\n",
      "Processed and saved batch 949/1042\n",
      "Processed and saved batch 950/1042\n",
      "Processed and saved batch 951/1042\n",
      "Processed and saved batch 952/1042\n",
      "Processed and saved batch 953/1042\n",
      "Processed and saved batch 954/1042\n",
      "Processed and saved batch 955/1042\n",
      "Processed and saved batch 956/1042\n",
      "Processed and saved batch 957/1042\n",
      "Processed and saved batch 958/1042\n",
      "Processed and saved batch 959/1042\n",
      "Processed and saved batch 960/1042\n",
      "Processed and saved batch 961/1042\n",
      "Processed and saved batch 962/1042\n",
      "Processed and saved batch 963/1042\n",
      "Processed and saved batch 964/1042\n",
      "Processed and saved batch 965/1042\n",
      "Processed and saved batch 966/1042\n",
      "Processed and saved batch 967/1042\n",
      "Processed and saved batch 968/1042\n",
      "Processed and saved batch 969/1042\n",
      "Processed and saved batch 970/1042\n",
      "Processed and saved batch 971/1042\n",
      "Processed and saved batch 972/1042\n",
      "Processed and saved batch 973/1042\n",
      "Processed and saved batch 974/1042\n",
      "Processed and saved batch 975/1042\n",
      "Processed and saved batch 976/1042\n",
      "Processed and saved batch 977/1042\n",
      "Processed and saved batch 978/1042\n",
      "Processed and saved batch 979/1042\n",
      "Processed and saved batch 980/1042\n",
      "Processed and saved batch 981/1042\n",
      "Processed and saved batch 982/1042\n",
      "Processed and saved batch 983/1042\n",
      "Processed and saved batch 984/1042\n",
      "Processed and saved batch 985/1042\n",
      "Processed and saved batch 986/1042\n",
      "Processed and saved batch 987/1042\n",
      "Processed and saved batch 988/1042\n",
      "Processed and saved batch 989/1042\n",
      "Processed and saved batch 990/1042\n",
      "Processed and saved batch 991/1042\n",
      "Processed and saved batch 992/1042\n",
      "Processed and saved batch 993/1042\n",
      "Processed and saved batch 994/1042\n",
      "Processed and saved batch 995/1042\n",
      "Processed and saved batch 996/1042\n",
      "Processed and saved batch 997/1042\n",
      "Processed and saved batch 998/1042\n",
      "Processed and saved batch 999/1042\n",
      "Processed and saved batch 1000/1042\n",
      "Processed and saved batch 1001/1042\n",
      "Processed and saved batch 1002/1042\n",
      "Processed and saved batch 1003/1042\n",
      "Processed and saved batch 1004/1042\n",
      "Processed and saved batch 1005/1042\n",
      "Processed and saved batch 1006/1042\n",
      "Processed and saved batch 1007/1042\n",
      "Processed and saved batch 1008/1042\n",
      "Processed and saved batch 1009/1042\n",
      "Processed and saved batch 1010/1042\n",
      "Processed and saved batch 1011/1042\n",
      "Processed and saved batch 1012/1042\n",
      "Processed and saved batch 1013/1042\n",
      "Processed and saved batch 1014/1042\n",
      "Processed and saved batch 1015/1042\n",
      "Processed and saved batch 1016/1042\n",
      "Processed and saved batch 1017/1042\n",
      "Processed and saved batch 1018/1042\n",
      "Processed and saved batch 1019/1042\n",
      "Processed and saved batch 1020/1042\n",
      "Processed and saved batch 1021/1042\n",
      "Processed and saved batch 1022/1042\n",
      "Processed and saved batch 1023/1042\n",
      "Processed and saved batch 1024/1042\n",
      "Processed and saved batch 1025/1042\n",
      "Processed and saved batch 1026/1042\n",
      "Processed and saved batch 1027/1042\n",
      "Processed and saved batch 1028/1042\n",
      "Processed and saved batch 1029/1042\n",
      "Processed and saved batch 1030/1042\n",
      "Processed and saved batch 1031/1042\n",
      "Processed and saved batch 1032/1042\n",
      "Processed and saved batch 1033/1042\n",
      "Processed and saved batch 1034/1042\n",
      "Processed and saved batch 1035/1042\n",
      "Processed and saved batch 1036/1042\n",
      "Processed and saved batch 1037/1042\n",
      "Processed and saved batch 1038/1042\n",
      "Processed and saved batch 1039/1042\n",
      "Processed and saved batch 1040/1042\n",
      "Processed and saved batch 1041/1042\n",
      "Processed and saved batch 1042/1042\n",
      "Data successfully saved in ../data/processed/sequences_strong/X_scaled_sequences.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假设 X_reshaped 是你要处理的数据\n",
    "# 创建保存处理后数据的 HDF5 文件\n",
    "output_file = \"../data/processed/sequences_strong/X_scaled_sequences.h5\"\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)  # 如果文件已经存在，删除它以重新创建\n",
    "\n",
    "batch_size = 100000\n",
    "fill_value = -1\n",
    "\n",
    "# 确定数据的形状\n",
    "num_samples, num_features = X_reshaped.shape\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size  # 向上取整\n",
    "\n",
    "# 初始化 HDF5 文件来存储处理后的数据\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    # 创建 HDF5 数据集，初始大小为 0，但允许动态扩展\n",
    "    X_scaled_dset = f.create_dataset(\"X_scaled\", shape=(0, num_features), maxshape=(None, num_features), dtype='float32', chunks=True)\n",
    "\n",
    "    for batch_index in range(num_batches):\n",
    "        start_idx = batch_index * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "\n",
    "        # 提取当前批次的数据\n",
    "        X_batch = X_reshaped[start_idx:end_idx]\n",
    "        valid_mask = ~(X_batch == fill_value).all(axis=1)\n",
    "\n",
    "        # 对有效数据进行标准化\n",
    "        X_valid = X_batch[valid_mask]\n",
    "        if X_valid.size > 0:\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled_valid = scaler.fit_transform(X_valid)\n",
    "\n",
    "            # 将标准化后的数据放回原数组的位置\n",
    "            X_batch_scaled = np.full_like(X_batch, fill_value, dtype=np.float32)\n",
    "            X_batch_scaled[valid_mask] = X_scaled_valid.astype(np.float32)\n",
    "        else:\n",
    "            # 如果批次中全是填充值\n",
    "            X_batch_scaled = X_batch.astype(np.float32)\n",
    "\n",
    "        # 将处理后的数据追加写入 HDF5 文件\n",
    "        X_scaled_dset.resize(X_scaled_dset.shape[0] + X_batch_scaled.shape[0], axis=0)\n",
    "        X_scaled_dset[-X_batch_scaled.shape[0]:] = X_batch_scaled\n",
    "\n",
    "        # 清除当前批次的内存\n",
    "        del X_batch, X_batch_scaled\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Processed and saved batch {batch_index + 1}/{num_batches}\")\n",
    "\n",
    "print(f\"Data successfully saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_scaled shape: (104144350, 42)\n"
     ]
    }
   ],
   "source": [
    "# load the processed strong dataset\n",
    "import h5py\n",
    "\n",
    "h5_file = '../data/processed/sequences_strong/X_scaled_sequences.h5'\n",
    "with h5py.File(h5_file, 'r') as f:\n",
    "    X_scaled = f['X_scaled'][:]\n",
    "\n",
    "print(f\"X_scaled shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (10414435, 10, 42)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = X_sequences.shape[1]\n",
    "num_features = X_sequences.shape[2]\n",
    "print(f\"Original shape: {X_sequences.shape}\")\n",
    "X_sequences_scaled = X_scaled.reshape(-1, sequence_length, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop large arrays to free up memory\n",
    "del X_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences_scaled, y_sequences, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
      " [ 0.47022963  0.10492861 -0.09214044 -0.07865118 -0.43732694  0.5350597\n",
      "   0.56004095 -1.2447524   0.95250577  0.0100005  -0.81292903 -0.69225496\n",
      "  -0.69265455 -0.9078149  -0.4405292   1.9853489   1.8236568  -0.3269986\n",
      "   0.02236627 -0.5862821  -0.05303842  0.8118936   1.556552    1.2315133\n",
      "   1.2642865  -1.0725352   0.6655371   0.99565786  1.2929624   1.5422623\n",
      "   1.5448405  -1.3956115   1.6723889  -0.9870998   0.41041303  0.90990096\n",
      "  -0.0929329   0.12771937 -1.0160348  -1.1008123   1.5104326   0.40674934]\n",
      " [ 0.47022963 -0.42975318 -0.09214044 -0.07865118  0.49957255  0.5350597\n",
      "   0.56004095 -0.7012178  -0.04895473  0.0100005  -0.81292903 -0.69225496\n",
      "   0.21487206  0.13984852 -0.45932868  0.39592195  0.2799975  -0.3269986\n",
      "   0.02236627 -1.0039365  -0.05303842  0.8118936   1.2493438  -0.98221135\n",
      "  -0.07577458  0.65559924  0.9656104  -1.4606218  -1.4891344   0.18282443\n",
      "   1.7290478   1.0430375   1.5575345  -0.24747966 -0.867499   -1.3788828\n",
      "  -0.61017334 -0.8629234   1.6398118   0.9281647  -0.71213555  1.0768104 ]\n",
      " [ 0.47022963  0.29176635 -0.09214044 -0.07865118  1.0857505   0.5350597\n",
      "   0.56004095  1.4729211  -0.54968494  0.0100005  -0.81292903 -0.69225496\n",
      "   0.21487206 -0.29667792  0.12665139  1.5879922   1.437742   -0.3269986\n",
      "   0.02236627 -0.47476903 -0.05303842  0.8118936   0.7424502   0.49360496\n",
      "  -0.02091828 -0.59118265  1.1656592  -0.3251718   1.0078168  -1.3993464\n",
      "  -1.4255023   0.3129482  -1.4133666   1.3587662  -1.1888453  -0.11968142\n",
      "  -1.096988    0.63805056 -0.7084368   1.84568    -0.1224746   0.708645  ]]\n"
     ]
    }
   ],
   "source": [
    "# print first row of X_train\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train and test data\n",
    "np.save('../data/train/timeSeries_StrongAction/X_train.npy', X_train)\n",
    "np.save('../data/test/timeSeries_StrongAction/X_test.npy', X_test)\n",
    "np.save('../data/train/timeSeries_StrongAction/y_train.npy', y_train)\n",
    "np.save('../data/test/timeSeries_StrongAction/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test data\n",
    "\n",
    "# timeSeries_StrongAction\n",
    "# X_test = np.load('../data/test/timeSeries_StrongAction/X_test.npy')\n",
    "# y_test = np.load('../data/test/timeSeries_StrongAction/y_test.npy')\n",
    "\n",
    "# numerical\n",
    "X_train = np.load('../data/train/numerical/X_train.npy')\n",
    "X_test = np.load('../data/test/numerical/X_test.npy')\n",
    "y_train = np.load('../data/train/numerical/y_train.npy')\n",
    "y_test = np.load('../data/test/numerical/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples in train data: 1580865\n",
      "Number of positive samples in train data: 11979409\n",
      "Number of negative samples in test data: 395685\n",
      "Number of positive samples in test data: 2994384\n"
     ]
    }
   ],
   "source": [
    "# check how many negative samples in train data\n",
    "print(f\"Number of negative samples in train data: {np.sum(y_train == 0)}\")\n",
    "print(f\"Number of positive samples in train data: {np.sum(y_train == 1)}\")\n",
    "\n",
    "# check how many negative samples in test data\n",
    "print(f\"Number of negative samples in test data: {np.sum(y_test == 0)}\")\n",
    "print(f\"Number of positive samples in test data: {np.sum(y_test == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples in balanced test data: 395685\n",
      "Number of positive samples in balanced test data: 395685\n",
      "Shape of balanced test data: (791370, 92)\n"
     ]
    }
   ],
   "source": [
    "# make test data balanced\n",
    "# get indices of positive and negative samples\n",
    "positive_indices = np.where(y_test == 1)[0]\n",
    "negative_indices = np.where(y_test == 0)[0]\n",
    "num_positive_samples = len(positive_indices)\n",
    "num_negative_samples = len(negative_indices)\n",
    "\n",
    "# downsample positive samples\n",
    "np.random.seed(42)\n",
    "downsampled_positive_indices = np.random.choice(positive_indices, size=num_negative_samples, replace=False)\n",
    "balanced_indices = np.concatenate([downsampled_positive_indices, negative_indices])\n",
    "\n",
    "# shuffle the indices\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "# get balanced test data\n",
    "y_test_balanced = y_test[balanced_indices]\n",
    "X_test_balanced = X_test[balanced_indices]\n",
    "\n",
    "# check how many negative samples in balanced test data\n",
    "print(f\"Number of negative samples in balanced test data: {np.sum(y_test_balanced == 0)}\")\n",
    "print(f\"Number of positive samples in balanced test data: {np.sum(y_test_balanced == 1)}\")\n",
    "print(f\"Shape of balanced test data: {X_test_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloader for big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# data loader\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X_file, y_file, batch_size=2048, shuffle=True):\n",
    "        self.X_file = X_file\n",
    "        self.y_file = y_file\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.X_data = np.load(X_file, mmap_mode='r')\n",
    "        self.y_data = np.load(y_file, mmap_mode='r')\n",
    "        self.indexes = np.arange(self.X_data.shape[0])\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        X_batch = self.X_data[indexes]\n",
    "        y_batch = self.y_data[indexes]\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "\n",
    "# DataGenerator\n",
    "batch_size = 2048 \n",
    "train_generator = DataGenerator('../data/train/timeSeries_StrongAction/X_train.npy',\n",
    "                                '../data/train/timeSeries_StrongAction/y_train.npy',\n",
    "                                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.899728e+06 3.049300e+05 0.000000e+00 0.000000e+00 1.107000e+02\n",
      " 2.021000e+03 6.000000e+00 2.900000e+01 1.000000e+00 1.000000e+00\n",
      " 1.000000e+00 1.000000e+00 7.000000e+00 9.100000e+01 5.150160e+05\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 2.649000e+03\n",
      " 9.000000e+01 2.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 1.570000e+02 3.740000e+02 4.390000e+02\n",
      " 2.440000e+02 2.670000e+02 7.900000e+01 9.600000e+01 3.270000e+02\n",
      " 2.300000e+01 3.350000e+02 1.500000e+02 2.890000e+02 2.140000e+02\n",
      " 1.700000e+01 1.400000e+02 3.410000e+02 2.240000e+02 3.800000e+01\n",
      " 1.920000e+02 6.700000e+01]\n"
     ]
    }
   ],
   "source": [
    "# check the first row of X_train original data\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaler for numerical datasets ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.86144690e-01 -6.05450079e-01 -9.13440240e-02 -4.68720280e-02\n",
      "  4.27310751e-01  7.30503193e-01 -2.53134537e-01  1.37850369e+00\n",
      " -1.06351936e+00  1.29023165e-02  1.23677621e+00  1.48536274e+00\n",
      "  2.25564676e-01  4.87131358e-01  8.35271665e+00 -1.14038260e+00\n",
      " -1.21438255e+00 -3.19607298e-01  1.92575363e-02 -1.73346051e-01\n",
      " -3.36142412e-01 -2.07073786e-01 -4.40735413e-01 -1.46182195e-01\n",
      " -1.21424156e-01 -1.55078383e-01 -8.30348582e-03 -1.60364621e-01\n",
      " -1.32021271e-01 -1.92662417e-01 -1.91871986e-01 -2.28564512e-01\n",
      " -4.95732049e-02 -1.49748348e-02 -8.37291853e-02 -8.63709244e-02\n",
      " -2.09999108e-02 -1.12932753e-01 -7.01794547e-02 -1.21949498e-01\n",
      " -5.55791920e-02 -4.70676753e-02 -1.44020134e-01 -1.04215500e-01\n",
      "  1.33354404e-01  7.78485506e-02 -1.07414871e-02 -3.60643419e-02\n",
      "  1.90827085e-02 -8.73269168e-02 -1.15284837e-01  8.30226474e-03\n",
      " -1.16478862e-02 -4.96531054e-02 -1.16183149e-01  4.50318381e-02\n",
      "  1.94609584e-02 -1.13900858e-01  1.83505982e-02 -1.04056432e-02\n",
      " -1.11259628e-01 -1.32998250e-02  2.64435117e-03 -7.24173406e-02\n",
      " -6.83013948e-02 -5.21657264e-02  1.62139417e-02 -2.62569809e-02\n",
      "  6.86884278e-02 -4.64546842e-02 -2.15820171e-02 -1.52314104e-03\n",
      " -3.89377499e-01  1.31569621e+00  1.85627718e+00  2.00776767e-01\n",
      "  4.14061489e-01 -1.04845463e+00 -8.87436429e-01  9.54323737e-01\n",
      " -1.37071754e+00  1.02115878e+00 -3.86864186e-01  6.68599486e-01\n",
      "  1.60741897e-01 -1.29100846e+00 -3.26909664e-01  1.20810897e+00\n",
      "  3.76610690e-01 -1.04304707e+00  1.76270298e-01 -7.54481372e-01]\n"
     ]
    }
   ],
   "source": [
    "# check again after scaling\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "52970/52970 [==============================] - 142s 3ms/step - loss: 0.2987 - accuracy: 0.8894 - val_loss: 0.2962 - val_accuracy: 0.8916\n",
      "Epoch 2/5\n",
      "52970/52970 [==============================] - 142s 3ms/step - loss: 0.2924 - accuracy: 0.8910 - val_loss: 0.2936 - val_accuracy: 0.8919\n",
      "Epoch 3/5\n",
      "52970/52970 [==============================] - 139s 3ms/step - loss: 0.2907 - accuracy: 0.8914 - val_loss: 0.2897 - val_accuracy: 0.8922\n",
      "Epoch 4/5\n",
      "52970/52970 [==============================] - 147s 3ms/step - loss: 0.2898 - accuracy: 0.8917 - val_loss: 0.2872 - val_accuracy: 0.8929\n",
      "Epoch 5/5\n",
      "52970/52970 [==============================] - 140s 3ms/step - loss: 0.2892 - accuracy: 0.8919 - val_loss: 0.2899 - val_accuracy: 0.8928\n"
     ]
    }
   ],
   "source": [
    "# training!\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('../model/baseline1.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=5,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline2 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Add, Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def create_model(input_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # 初始层\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # 第一层残差块\n",
    "    shortcut = x\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # 第二层残差块，调整 shortcut 的维度\n",
    "    shortcut = x\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # 使用 Dense 层改变 shortcut 的维度\n",
    "    shortcut = Dense(256)(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # 第三层残差块，继续调整 shortcut 的维度\n",
    "    shortcut = x\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    shortcut = Dense(128)(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # 第四层残差块，继续调整 shortcut 的维度\n",
    "    shortcut = x\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    shortcut = Dense(64)(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # 输出层\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 92)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          47616       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 512)         2048        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          262656      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 512)         2048        ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 512)          0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 512)          0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          131328      ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          65792       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 256)         1024        ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 256)          0           ['batch_normalization_4[0][0]',  \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 256)          0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          32896       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          16512       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          32896       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128)         512         ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 128)         512         ['dense_8[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 128)          0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128)          0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64)           8256        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 64)          256         ['dense_9[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64)           0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           4160        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           8256        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 64)          256         ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 64)          256         ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64)           0           ['batch_normalization_10[0][0]', \n",
      "                                                                  'batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 64)           0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            65          ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,015,937\n",
      "Trainable params: 1,010,177\n",
      "Non-trainable params: 5,760\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "batch_size = 2048\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used for computation: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# check current device\n",
    "a = tf.constant(1.0)\n",
    "print(\"Device used for computation:\", a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6622/6622 [==============================] - 59s 9ms/step - loss: 0.2980 - accuracy: 0.8893 - val_loss: 140561.9219 - val_accuracy: 0.4999\n",
      "Epoch 2/50\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.2867 - accuracy: 0.8924 - val_loss: 51242.3086 - val_accuracy: 0.4738\n",
      "Epoch 3/50\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.2840 - accuracy: 0.8933 - val_loss: 116584.2109 - val_accuracy: 0.4996\n",
      "Epoch 4/50\n",
      "6622/6622 [==============================] - 56s 8ms/step - loss: 0.2825 - accuracy: 0.8938 - val_loss: 284921.9375 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "6622/6622 [==============================] - 57s 9ms/step - loss: 0.2814 - accuracy: 0.8941 - val_loss: 58374.7227 - val_accuracy: 0.4735\n",
      "Epoch 6/50\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.2806 - accuracy: 0.8943 - val_loss: 37328.6328 - val_accuracy: 0.4755\n",
      "Epoch 7/50\n",
      "1456/6622 [=====>........................] - ETA: 42s - loss: 0.2798 - accuracy: 0.8945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_balanced\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/resnet_baseline3.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test_balanced, y_test_balanced),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save('../model/resnet_baseline3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\CV\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:44:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# 将数据转换为DMatrix格式\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_balanced, label=y_test_balanced)\n",
    "scale_pos_weight = 0.13 / 0.87\n",
    "\n",
    "# 设置XGBoost的参数，启用GPU加速并使用scale_pos_weight\n",
    "param = {\n",
    "    'max_depth': 10,\n",
    "    'eta': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'hist',        \n",
    "    'device': 'cuda', \n",
    "    'num_parallel_tree': 100, \n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight  # 调整正负样本权重\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "model = xgb.train(param, dtrain, num_boost_round=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\CV\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:45:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00    395685\n",
      "           1       0.50      1.00      0.67    395685\n",
      "\n",
      "    accuracy                           0.50    791370\n",
      "   macro avg       0.75      0.50      0.33    791370\n",
      "weighted avg       0.75      0.50      0.33    791370\n",
      "\n",
      "ROC-AUC Score: 0.4082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "y_pred_prob = model.predict(dtest)\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n",
    "\n",
    "print(classification_report(y_test_balanced, y_pred))\n",
    "auc = roc_auc_score(y_test_balanced, y_pred_prob)\n",
    "print(f'ROC-AUC Score: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 92)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 512)          47616       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 512)         2048        ['dense_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 512)          0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 512)          0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 512)          262656      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 512)         2048        ['dense_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 512)          0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 512)          0           ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 512)          262656      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 512)         2048        ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 512)          0           ['batch_normalization_14[0][0]', \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 512)          0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 256)          131328      ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 256)         1024        ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 256)          0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 256)          0           ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 256)          65792       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 256)          131328      ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 256)         1024        ['dense_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 256)         1024        ['dense_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 256)          0           ['batch_normalization_16[0][0]', \n",
      "                                                                  'batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 256)          0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 128)          32896       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 128)         512         ['dense_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 128)          0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 128)          16512       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 128)          32896       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 128)         512         ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 128)         512         ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 128)          0           ['batch_normalization_19[0][0]', \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 128)          0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 64)           8256        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 64)          256         ['dense_22[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 64)           0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64)           0           ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 64)           4160        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 64)           8256        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 64)          256         ['dense_23[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 64)          256         ['dense_24[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 64)           0           ['batch_normalization_22[0][0]', \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 64)           0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 1)            65          ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,015,937\n",
      "Trainable params: 1,010,177\n",
      "Non-trainable params: 5,760\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Add, Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def create_model(input_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # 初始层\n",
    "    x = Dense(512)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # 第一层残差块\n",
    "    shortcut = x\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # 后续残差块，逐渐减少维度\n",
    "    for units in [256, 128, 64]:\n",
    "        shortcut = x\n",
    "        x = Dense(units)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(units)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        # 调整shortcut维度\n",
    "        shortcut = Dense(units)(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "        \n",
    "        x = Add()([x, shortcut])\n",
    "        x = Activation('relu')(x)\n",
    "    \n",
    "    # 输出层\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "def focal_loss(alpha=0.13, gamma=2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1) \\\n",
    "               - (1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0)\n",
    "        return K.mean(loss)\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=focal_loss(alpha=0.13, gamma=2), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6622/6622 [==============================] - 56s 8ms/step - loss: 0.0294 - accuracy: 0.7557 - val_loss: 0.0628 - val_accuracy: 0.7324 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.0290 - accuracy: 0.7595 - val_loss: 0.0644 - val_accuracy: 0.7410 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "6622/6622 [==============================] - 54s 8ms/step - loss: 0.0288 - accuracy: 0.7624 - val_loss: 0.0551 - val_accuracy: 0.7169 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "6622/6622 [==============================] - 54s 8ms/step - loss: 0.0287 - accuracy: 0.7639 - val_loss: 0.0671 - val_accuracy: 0.7437 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "6622/6622 [==============================] - 54s 8ms/step - loss: 0.0286 - accuracy: 0.7651 - val_loss: 0.0576 - val_accuracy: 0.7263 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.0285 - accuracy: 0.7664 - val_loss: 0.0558 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.0285 - accuracy: 0.7665 - val_loss: 0.0663 - val_accuracy: 0.7457 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.0284 - accuracy: 0.7673 - val_loss: 0.0625 - val_accuracy: 0.7446 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.0282 - accuracy: 0.7691 - val_loss: 0.0584 - val_accuracy: 0.7357 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "6622/6622 [==============================] - 54s 8ms/step - loss: 0.0282 - accuracy: 0.7696 - val_loss: 0.0634 - val_accuracy: 0.7482 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "6622/6622 [==============================] - 56s 8ms/step - loss: 0.0281 - accuracy: 0.7699 - val_loss: 0.0630 - val_accuracy: 0.7481 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "6622/6622 [==============================] - 54s 8ms/step - loss: 0.0281 - accuracy: 0.7704 - val_loss: 0.0623 - val_accuracy: 0.7469 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "6622/6622 [==============================] - 55s 8ms/step - loss: 0.0281 - accuracy: 0.7709 - val_loss: 0.0579 - val_accuracy: 0.7372 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test_balanced, y_test_balanced),\n",
    "                    epochs=100,\n",
    "                    batch_size=2048,\n",
    "                    callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_columns = ['helpful', 'funny', 'hours', 'year', 'month', 'day', 'weekday',\n",
    "                   'win', 'mac', 'linux', 'rating', 'positive_ratio', 'user_reviews', 'price_final',\n",
    "                   'price_original', 'discount', 'steam_deck', 'release_days', 'products', 'reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_vector_array = np.stack(data['title_vector'].values)\n",
    "max_len = max(len(x) for x in data['tags_enc'].values)\n",
    "tags_enc_array = np.array([np.pad(x, (0, max_len - len(x)), 'constant') for x in data['tags_enc'].values])\n",
    "\n",
    "X_numeric = data[final_feature_columns].values\n",
    "X_combined = np.hstack((X_numeric, title_vector_array, tags_enc_array))\n",
    "\n",
    "user_id = data['user_id'].values\n",
    "app_id = data['app_id'].values\n",
    "\n",
    "y = data['is_recommended'].values\n",
    "\n",
    "X_train_combined, X_test_combined, y_train, y_test, X_train_user, X_test_user, X_train_item, X_test_item = train_test_split(\n",
    "    X_combined, y, user_id, app_id, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_features = X_train_combined[:, :len(final_feature_columns)]\n",
    "X_test_features = X_test_combined[:, :len(final_feature_columns)]\n",
    "\n",
    "# X_train_user 包含 user_id，X_train_item 包含 app_id，X_train_features 包含数值特征\n",
    "# X_test_user 包含测试集的 user_id，X_test_item 包含测试集的 app_id，X_test_features 包含测试集的数值特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test data\n",
    "np.save('../data/test/ncf/realworld/X_test_features.npy', X_test_features)\n",
    "np.save('../data/test/ncf/realworld/X_test_user.npy', X_test_user)\n",
    "np.save('../data/test/ncf/realworld/X_test_item.npy', X_test_item)\n",
    "np.save('../data/test/ncf/realworld/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 6851751\n",
      "Number of unique items: 34841\n"
     ]
    }
   ],
   "source": [
    "# record number of unique values for\n",
    "num_users = len(data['user_id'].unique())\n",
    "num_items = len(data['app_id'].unique())\n",
    "print(f\"Number of unique users: {num_users}\")\n",
    "print(f\"Number of unique items: {num_items}\")\n",
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the test data for [X_test_user, X_test_item, X_test_features], y_test\n",
    "# get indices of positive and negative samples\n",
    "positive_indices = np.where(y_test == 1)[0]\n",
    "negative_indices = np.where(y_test == 0)[0]\n",
    "num_positive_samples = len(positive_indices)\n",
    "num_negative_samples = len(negative_indices)\n",
    "\n",
    "# downsample positive samples\n",
    "np.random.seed(42)\n",
    "downsampled_positive_indices = np.random.choice(positive_indices, size=num_negative_samples, replace=False)\n",
    "balanced_indices = np.concatenate([downsampled_positive_indices, negative_indices])\n",
    "\n",
    "# shuffle the indices\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "# get balanced test data\n",
    "y_test_balanced = y_test[balanced_indices]\n",
    "X_test_user_balanced = X_test_user[balanced_indices]\n",
    "X_test_item_balanced = X_test_item[balanced_indices]\n",
    "X_test_features_balanced = X_test_features[balanced_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "X_train_user = np.load('../data/train/ncf/X_train_user.npy')\n",
    "X_train_item = np.load('../data/train/ncf/X_train_item.npy')\n",
    "X_train_features = np.load('../data/train/ncf/X_train_features.npy')\n",
    "y_train = np.load('../data/train/ncf/y_train.npy')\n",
    "\n",
    "X_test_user = np.load('../data/test/ncf/realworld/X_test_user.npy')\n",
    "X_test_item = np.load('../data/test/ncf/realworld/X_test_item.npy')\n",
    "X_test_features = np.load('../data/test/ncf/realworld/X_test_features.npy')\n",
    "y_test = np.load('../data/test/ncf/realworld/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "def focal_loss(alpha=0.13, gamma=2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1) \\\n",
    "               - (1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0)\n",
    "        return K.mean(loss)\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# cheat\n",
    "num_users = 6851751\n",
    "num_items = 34841\n",
    "embedding_size = 8 \n",
    "\n",
    "# user embedding\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(input_dim=num_users+1, output_dim=embedding_size, name='user_embedding')(user_input)\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "\n",
    "# item embedding\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "item_embedding = Embedding(input_dim=num_items+1, output_dim=embedding_size, name='item_embedding')(item_input)\n",
    "item_flatten = Flatten()(item_embedding)\n",
    "\n",
    "features_input = Input(shape=(X_train_features.shape[1],), name='features_input')\n",
    "concat = Concatenate()([user_flatten, item_flatten, features_input])\n",
    "dense_1 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(concat)\n",
    "dropout_1 = Dropout(0.5)(dense_1)\n",
    "dense_2 = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_2)\n",
    "dense_3 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(dropout_2)\n",
    "dense_4 = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(dense_3)\n",
    "output = Dense(1, activation='sigmoid', name='output')(dense_4)\n",
    "\n",
    "model = Model(inputs=[user_input, item_input, features_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " user_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " item_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " user_embedding (Embedding)     (None, 1, 8)         54814016    ['user_input[0][0]']             \n",
      "                                                                                                  \n",
      " item_embedding (Embedding)     (None, 1, 8)         278736      ['item_input[0][0]']             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 8)            0           ['user_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 8)            0           ['item_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " features_input (InputLayer)    [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 36)           0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'features_input[0][0]']         \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          9472        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          32896       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            33          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,145,489\n",
      "Trainable params: 55,145,489\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\CV\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     class_weight='balanced',  # 自动平衡权重\n",
    "#     classes=np.unique(y_train),\n",
    "#     y=y_train\n",
    "# )\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss=focal_loss(alpha=0.13, gamma=2), metrics=['accuracy'])\n",
    "# model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all training and testing data\n",
    "train_output_dir = \"../data/train/ncf\"\n",
    "test_output_dir = \"../data/test/ncf\"\n",
    "if not os.path.exists(train_output_dir):\n",
    "    os.makedirs(train_output_dir)\n",
    "if not os.path.exists(test_output_dir):\n",
    "    os.makedirs(test_output_dir)\n",
    "\n",
    "np.save(os.path.join(train_output_dir, 'X_train_user.npy'), X_train_user)\n",
    "np.save(os.path.join(train_output_dir, 'X_train_item.npy'), X_train_item)\n",
    "np.save(os.path.join(train_output_dir, 'X_train_features.npy'), X_train_features)\n",
    "np.save(os.path.join(train_output_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(test_output_dir, 'X_test_user.npy'), X_test_user_balanced)\n",
    "np.save(os.path.join(test_output_dir, 'X_test_item.npy'), X_test_item_balanced)\n",
    "np.save(os.path.join(test_output_dir, 'X_test_features.npy'), X_test_features_balanced)\n",
    "np.save(os.path.join(test_output_dir, 'y_test.npy'), y_test_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6622/6622 [==============================] - 109s 16ms/step - loss: 0.3912 - accuracy: 0.8343 - val_loss: 0.0379 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "6622/6622 [==============================] - 108s 16ms/step - loss: 0.0375 - accuracy: 0.8823 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "6622/6622 [==============================] - 108s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "6622/6622 [==============================] - 108s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 1.2500e-04\n",
      "Epoch 19/100\n",
      "6622/6622 [==============================] - 108s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "6622/6622 [==============================] - 108s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 6.2500e-05\n",
      "Epoch 24/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 6.2500e-05\n",
      "Epoch 25/100\n",
      "6622/6622 [==============================] - 108s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 6.2500e-05\n",
      "Epoch 26/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 6.2500e-05\n",
      "Epoch 27/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 6.2500e-05\n",
      "Epoch 28/100\n",
      "6622/6622 [==============================] - 106s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 3.1250e-05\n",
      "Epoch 29/100\n",
      "6622/6622 [==============================] - 107s 16ms/step - loss: 0.0374 - accuracy: 0.8834 - val_loss: 0.0374 - val_accuracy: 0.8833 - lr: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint('./model/best_ncf_model.h5', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_user, X_train_item, X_train_features],\n",
    "    y_train,\n",
    "    validation_data=([X_test_user, X_test_item, X_test_features], y_test),\n",
    "    epochs=100,\n",
    "    batch_size=2048,\n",
    "    # class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 10, 42)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 256)           306176    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 256)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,617\n",
      "Trainable params: 511,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking\n",
    "\n",
    "sequence_length = 10\n",
    "fill_value = -1\n",
    "num_features = 42\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=fill_value, input_shape=(sequence_length, num_features)))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4068/4068 [==============================] - 508s 124ms/step - loss: 0.3133 - accuracy: 0.8833 - val_loss: 0.3023 - val_accuracy: 0.8858\n",
      "Epoch 2/50\n",
      "4068/4068 [==============================] - 494s 121ms/step - loss: 0.3030 - accuracy: 0.8855 - val_loss: 0.2978 - val_accuracy: 0.8870\n",
      "Epoch 3/50\n",
      "4068/4068 [==============================] - 496s 122ms/step - loss: 0.2997 - accuracy: 0.8864 - val_loss: 0.2953 - val_accuracy: 0.8877\n",
      "Epoch 4/50\n",
      "4068/4068 [==============================] - 492s 121ms/step - loss: 0.2976 - accuracy: 0.8870 - val_loss: 0.2942 - val_accuracy: 0.8881\n",
      "Epoch 5/50\n",
      "4068/4068 [==============================] - 493s 121ms/step - loss: 0.2961 - accuracy: 0.8874 - val_loss: 0.2932 - val_accuracy: 0.8884\n",
      "Epoch 6/50\n",
      "4068/4068 [==============================] - 494s 121ms/step - loss: 0.2949 - accuracy: 0.8877 - val_loss: 0.2926 - val_accuracy: 0.8885\n",
      "Epoch 7/50\n",
      "4068/4068 [==============================] - 473s 116ms/step - loss: 0.2939 - accuracy: 0.8882 - val_loss: 0.2923 - val_accuracy: 0.8886\n",
      "Epoch 8/50\n",
      "4068/4068 [==============================] - 466s 115ms/step - loss: 0.2931 - accuracy: 0.8884 - val_loss: 0.2919 - val_accuracy: 0.8887\n",
      "Epoch 9/50\n",
      "4068/4068 [==============================] - 465s 114ms/step - loss: 0.2922 - accuracy: 0.8886 - val_loss: 0.2919 - val_accuracy: 0.8888\n",
      "Epoch 10/50\n",
      "4068/4068 [==============================] - 469s 115ms/step - loss: 0.2915 - accuracy: 0.8888 - val_loss: 0.2917 - val_accuracy: 0.8889\n",
      "Epoch 11/50\n",
      "4068/4068 [==============================] - 467s 115ms/step - loss: 0.2908 - accuracy: 0.8891 - val_loss: 0.2915 - val_accuracy: 0.8889\n",
      "Epoch 12/50\n",
      "4068/4068 [==============================] - 467s 115ms/step - loss: 0.2902 - accuracy: 0.8892 - val_loss: 0.2915 - val_accuracy: 0.8889\n",
      "Epoch 13/50\n",
      "4068/4068 [==============================] - 467s 115ms/step - loss: 0.2896 - accuracy: 0.8894 - val_loss: 0.2918 - val_accuracy: 0.8890\n",
      "Epoch 14/50\n",
      "4068/4068 [==============================] - 467s 115ms/step - loss: 0.2889 - accuracy: 0.8896 - val_loss: 0.2919 - val_accuracy: 0.8889\n",
      "Epoch 15/50\n",
      "4068/4068 [==============================] - 468s 115ms/step - loss: 0.2884 - accuracy: 0.8898 - val_loss: 0.2918 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "4068/4068 [==============================] - 467s 115ms/step - loss: 0.2878 - accuracy: 0.8898 - val_loss: 0.2921 - val_accuracy: 0.8888\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_lstm_model_with_strongActions.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=50,\n",
    "                    callbacks=[early_stopping, model_checkpoint],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "\n",
    "plt.xticks(np.arange(0, len(history.history['loss']), step=1))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "\n",
    "plt.xticks(np.arange(0, len(history.history['accuracy']), step=1))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下暂时是resnet的cm分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105940/105940 [==============================] - 223s 2ms/step - loss: 0.2720 - accuracy: 0.8972\n",
      "测试集损失：0.2720302939414978\n",
      "测试集准确率：0.8971896767616272\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "from keras.models import load_model\n",
    "\n",
    "best_model = load_model('../model/resnet_baseline2.h5')\n",
    "\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"val loss: {loss}\")\n",
    "print(f\"val acc: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105940/105940 [==============================] - 182s 2ms/step\n",
      "[[  91353  304332]\n",
      " [  44202 2950182]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.23      0.34    395685\n",
      "           1       0.91      0.99      0.94   2994384\n",
      "\n",
      "    accuracy                           0.90   3390069\n",
      "   macro avg       0.79      0.61      0.64   3390069\n",
      "weighted avg       0.88      0.90      0.87   3390069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# plot cm\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Recommendation function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
